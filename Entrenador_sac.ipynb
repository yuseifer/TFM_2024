{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1BjbInZXnhdEAigjScdWges3OTze-tbi3",
      "authorship_tag": "ABX9TyOsvPA4rUOA7vGBrQDcz4aa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuseifer/TFM_2024/blob/main/Entrenador_sac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3EUxIs-49Uo",
        "outputId": "f60bd600-76c0-4b96-d4ce-14f420b8211b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.26.4)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=fb86d05cd27b89f442a4cffdbf111c47c60e86f6c8280eb9ad5de2af02b15770\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.19.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n",
            "Collecting pybullet==3.2.6\n",
            "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.6\n",
            "Collecting stable_baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable_baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (2.4.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (2.17.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (4.66.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (13.8.1)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable_baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (10.4.0)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (2.32.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable_baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable_baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable_baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable_baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable_baselines3[extra]) (6.4.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable_baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable_baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3[extra]) (1.3.0)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446661 sha256=b5fd635c0835c2586056abaeab52d265212a22716d97149f9ce01b293e1d3232\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable_baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-1.3.0 stable_baselines3-2.3.2\n",
            "Collecting shimmy==1.2.1\n",
            "  Downloading Shimmy-1.2.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy==1.2.1) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy==1.2.1) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy==1.2.1) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy==1.2.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy==1.2.1) (0.0.4)\n",
            "Downloading Shimmy-1.2.1-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: shimmy\n",
            "  Attempting uninstall: shimmy\n",
            "    Found existing installation: Shimmy 1.3.0\n",
            "    Uninstalling Shimmy-1.3.0:\n",
            "      Successfully uninstalled Shimmy-1.3.0\n",
            "Successfully installed shimmy-1.2.1\n",
            "Collecting gymnasium==0.28.1\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (1.26.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (0.0.4)\n",
            "Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: jax-jumpy, gymnasium\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 0.29.1\n",
            "    Uninstalling gymnasium-0.29.1:\n",
            "      Successfully uninstalled gymnasium-0.29.1\n",
            "Successfully installed gymnasium-0.28.1 jax-jumpy-1.0.0\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.17.3\n",
        "!pip install pybullet==3.2.6\n",
        "!pip install stable_baselines3[extra]\n",
        "!pip install shimmy==1.2.1\n",
        "!pip install gymnasium==0.28.1\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ant_custom.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhyzKzwB5G0x",
        "outputId": "5a2b8871-3bef-40db-f818-5120d4edb57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ant_custom.zip\n",
            "   creating: ant_custom/\n",
            "   creating: ant_custom/CustomAnt_Env/\n",
            "   creating: ant_custom/CustomAnt_Env/envs/\n",
            "  inflating: ant_custom/CustomAnt_Env/envs/AntCustomEnvEmpty.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/envs/__init__.py  \n",
            "   creating: ant_custom/CustomAnt_Env/resources/\n",
            "  inflating: ant_custom/CustomAnt_Env/resources/antcustom.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/Cil_obs.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/goal.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/plane.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/simplegoal.urdf  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/simpleobstacle.urdf  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/simpleplane.urdf  \n",
            "  inflating: ant_custom/CustomAnt_Env/__init__.py  \n",
            "  inflating: ant_custom/setup.py     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import pybullet, pybullet_envs\n",
        "import torch as th\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "#Importamos librerías para realizar el multiprocesos y normalización del entorno\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "import gym\n",
        "import sys\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "sys.path.append('/content/ant_custom')\n",
        "import CustomAnt_Env"
      ],
      "metadata": {
        "id": "fwjU16X25QTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#We begin with training but first we set some hyperparameters\n",
        "SEED                = 42\n",
        "NUM_ENVS            = 8\n",
        "ENV_ID              = 'AntCustomEnv-v0'\n",
        "HIDDEN_SIZE         = 256\n",
        "LEARNING_RATE       = 1e-4\n",
        "GAMMA               = 0.99\n",
        "GAE_LAMBDA          = 0.95\n",
        "PPO_EPSILON         = 0.2\n",
        "CRITIC_DISCOUNT     = 0.5\n",
        "ENTROPY_BETA        = 0.001\n",
        "PPO_STEPS           = 256\n",
        "MINI_BATCH_SIZE     = 64\n",
        "PPO_EPOCHS          = 10\n",
        "TEST_EPOCHS         = 10\n",
        "NUM_TESTS           = 10\n",
        "TARGET_REWARD       = 50\n",
        "MAX_STEPS           = 10000\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "b2KADT6r5T1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the env\n",
        "env = gym.make(ENV_ID)\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape\n",
        "\n",
        "a_size = env.action_space"
      ],
      "metadata": {
        "id": "TEPL6hsh5VrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_AVERAGE_SCORE = 50e6\n",
        "#Definimos la arquitectura de la red\n",
        "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU, net_arch=[512, 512])\n",
        "#policy_kwargs = dict(activation_fn=th.nn.Tanh, net_arch=[512, 512, 264])"
      ],
      "metadata": {
        "id": "cZo2Niud5X5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluar_modelo(model, num_tests, env):\n",
        "    rewards = []\n",
        "    for i in range(num_tests):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    mean_reward = np.mean(rewards)\n",
        "    std_reward = np.std(rewards)\n",
        "    return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "eLL_bzAX5hHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "# Crear el callback de evaluación, evaluando cada 10,000 steps y guardando el mejor modelo\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model/',\n",
        "                             log_path='./logs/eval/', eval_freq=10000,\n",
        "                             deterministic=True, render=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UIoS_hz5iA7",
        "outputId": "59bc6876-b59b-4c37-da75-b868d877fa20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "path_save = \"/content/drive/MyDrive/Ant_custom/\"\n",
        "\n",
        "iteraciones = []\n",
        "recompensa_promedio=[]\n",
        "\n",
        "os.makedirs(path_save, exist_ok=True)\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive/Ant_custom/SAC_Ant_2.zip\"):\n",
        "    model = SAC.load(\"/content/drive/MyDrive/Ant_custom/SAC_Ant_2.zip\", env = env)\n",
        "    print(\"Model loaded\")\n",
        "else:\n",
        "    model = SAC('MlpPolicy', env,learning_rate=3e-4,policy_kwargs=policy_kwargs, verbose=1, batch_size=64)\n",
        "    print(\"Model created\")\n",
        "\n",
        "\n",
        "mean_rewards = []\n",
        "for i in range(10):\n",
        "  print(\"Training itteration \",i)\n",
        "  model.learn(total_timesteps=100000,log_interval = 10,callback=eval_callback)\n",
        "  # Save the agent\n",
        "  model.save(path_save+\"SAC_Ant_obs\")\n",
        "  mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)\n",
        "  mean_rewards.append(mean_reward)\n",
        "  print(f'mean reward: {mean_reward}, std reward: +/-{std_reward}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "del model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUc15U45rYE",
        "outputId": "190db63b-a77f-428f-c6a5-260dff6fa6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Model created\n",
            "Training itteration  0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-60419.38 +/- 94305.33\n",
            "Episode length: 2052.00 +/- 3974.50\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 2.05e+03  |\n",
            "|    mean_reward     | -6.04e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.84e+03  |\n",
            "|    critic_loss     | 2.4       |\n",
            "|    ent_coef        | 0.0751    |\n",
            "|    ent_coef_loss   | -4.26     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 9899      |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-504396.67 +/- 225236.18\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -5.04e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.84e+03  |\n",
            "|    critic_loss     | 2.45e+03  |\n",
            "|    ent_coef        | 0.708     |\n",
            "|    ent_coef_loss   | 1.32      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 19899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-310695.54 +/- 225748.78\n",
            "Episode length: 8049.60 +/- 3902.80\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.05e+03  |\n",
            "|    mean_reward     | -3.11e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.64e+03  |\n",
            "|    critic_loss     | 176       |\n",
            "|    ent_coef        | 1.76      |\n",
            "|    ent_coef_loss   | 0.212     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 29899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-308123.50 +/- 314826.77\n",
            "Episode length: 6028.00 +/- 4865.91\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.03e+03  |\n",
            "|    mean_reward     | -3.08e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.18e+03  |\n",
            "|    critic_loss     | 2.13e+03  |\n",
            "|    ent_coef        | 2.36      |\n",
            "|    ent_coef_loss   | -0.47     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 39899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-189234.27 +/- 214069.79\n",
            "Episode length: 6026.20 +/- 4868.12\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.03e+03  |\n",
            "|    mean_reward     | -1.89e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.59e+03  |\n",
            "|    critic_loss     | 1.27e+03  |\n",
            "|    ent_coef        | 2.81      |\n",
            "|    ent_coef_loss   | 1.11      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 49899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-218442.23 +/- 212646.32\n",
            "Episode length: 8013.40 +/- 3975.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -2.18e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 453       |\n",
            "|    critic_loss     | 433       |\n",
            "|    ent_coef        | 2.49      |\n",
            "|    ent_coef_loss   | 1.14      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 59899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-362757.99 +/- 190598.37\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -3.63e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.24e+03  |\n",
            "|    critic_loss     | 1e+03     |\n",
            "|    ent_coef        | 2.93      |\n",
            "|    ent_coef_loss   | -2.63     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 69899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-212001.87 +/- 247633.96\n",
            "Episode length: 4072.80 +/- 4840.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.07e+03  |\n",
            "|    mean_reward     | -2.12e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.18e+03  |\n",
            "|    critic_loss     | 2.08e+03  |\n",
            "|    ent_coef        | 2.84      |\n",
            "|    ent_coef_loss   | -1.92     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 79899     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 8.63e+03 |\n",
            "|    ep_rew_mean     | -3.9e+05 |\n",
            "| time/              |          |\n",
            "|    episodes        | 10       |\n",
            "|    fps             | 62       |\n",
            "|    time_elapsed    | 1376     |\n",
            "|    total_timesteps | 86329    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 426      |\n",
            "|    critic_loss     | 964      |\n",
            "|    ent_coef        | 2.44     |\n",
            "|    ent_coef_loss   | 0.831    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 86228    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-269772.53 +/- 297417.44\n",
            "Episode length: 6092.00 +/- 4788.70\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.09e+03 |\n",
            "|    mean_reward     | -2.7e+05 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 90000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 677      |\n",
            "|    critic_loss     | 513      |\n",
            "|    ent_coef        | 2.3      |\n",
            "|    ent_coef_loss   | -0.468   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 89899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-220455.64 +/- 222004.94\n",
            "Episode length: 6067.00 +/- 4818.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.07e+03 |\n",
            "|    mean_reward     | -2.2e+05 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 100000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 753      |\n",
            "|    critic_loss     | 1.1e+03  |\n",
            "|    ent_coef        | 1.92     |\n",
            "|    ent_coef_loss   | 0.00872  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99899    |\n",
            "---------------------------------\n",
            "mean reward: -303972.97300319996, std reward: +/-220569.2806460831\n",
            "Training itteration  1\n",
            "Eval num_timesteps=10000, episode_reward=-499062.57 +/- 271438.15\n",
            "Episode length: 8014.00 +/- 3974.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -4.99e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.02e+03  |\n",
            "|    critic_loss     | 1.66e+03  |\n",
            "|    ent_coef        | 1.9       |\n",
            "|    ent_coef_loss   | -0.291    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 109799    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-137954.52 +/- 143385.58\n",
            "Episode length: 4212.20 +/- 4733.28\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.21e+03  |\n",
            "|    mean_reward     | -1.38e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 908       |\n",
            "|    critic_loss     | 1.49e+03  |\n",
            "|    ent_coef        | 2.22      |\n",
            "|    ent_coef_loss   | -0.238    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 119799    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-418394.73 +/- 224520.20\n",
            "Episode length: 8277.00 +/- 3448.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.28e+03  |\n",
            "|    mean_reward     | -4.18e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.17e+03  |\n",
            "|    critic_loss     | 1.77e+03  |\n",
            "|    ent_coef        | 3.47      |\n",
            "|    ent_coef_loss   | 0.395     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 129799    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-267348.48 +/- 226734.13\n",
            "Episode length: 6264.40 +/- 4591.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.26e+03  |\n",
            "|    mean_reward     | -2.67e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -433      |\n",
            "|    critic_loss     | 2.44e+03  |\n",
            "|    ent_coef        | 3.91      |\n",
            "|    ent_coef_loss   | 0.625     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 139799    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-328501.80 +/- 229127.59\n",
            "Episode length: 8004.80 +/- 3924.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8e+03     |\n",
            "|    mean_reward     | -3.29e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 530       |\n",
            "|    critic_loss     | 4.5e+03   |\n",
            "|    ent_coef        | 4.52      |\n",
            "|    ent_coef_loss   | 0.566     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 149799    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-425957.41 +/- 253503.93\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -4.26e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 361       |\n",
            "|    critic_loss     | 3.54e+03  |\n",
            "|    ent_coef        | 5.23      |\n",
            "|    ent_coef_loss   | 0.668     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 159799    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-531232.40 +/- 240366.01\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -5.31e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.07e+03 |\n",
            "|    critic_loss     | 4.32e+03  |\n",
            "|    ent_coef        | 5.33      |\n",
            "|    ent_coef_loss   | 1.44      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 169799    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 7.17e+03  |\n",
            "|    ep_rew_mean     | -2.85e+05 |\n",
            "| time/              |           |\n",
            "|    episodes        | 10        |\n",
            "|    fps             | 59        |\n",
            "|    time_elapsed    | 1200      |\n",
            "|    total_timesteps | 71741     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.49e+03 |\n",
            "|    critic_loss     | 1.03e+04  |\n",
            "|    ent_coef        | 5.21      |\n",
            "|    ent_coef_loss   | -0.216    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 171540    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-410793.48 +/- 193693.32\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -4.11e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.16e+03 |\n",
            "|    critic_loss     | 1.68e+04  |\n",
            "|    ent_coef        | 4.83      |\n",
            "|    ent_coef_loss   | 0.237     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 179799    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-543194.14 +/- 95248.90\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -5.43e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.48e+03 |\n",
            "|    critic_loss     | 1.23e+04  |\n",
            "|    ent_coef        | 4.43      |\n",
            "|    ent_coef_loss   | -0.0996   |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 189799    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-309377.66 +/- 294665.75\n",
            "Episode length: 8040.40 +/- 3921.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.04e+03  |\n",
            "|    mean_reward     | -3.09e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.04e+03 |\n",
            "|    critic_loss     | 2.73e+03  |\n",
            "|    ent_coef        | 5.05      |\n",
            "|    ent_coef_loss   | 0.479     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 199799    |\n",
            "----------------------------------\n",
            "mean reward: -577441.3401329999, std reward: +/-174106.23206265003\n",
            "Training itteration  2\n",
            "Eval num_timesteps=10000, episode_reward=-306552.35 +/- 241225.83\n",
            "Episode length: 6280.20 +/- 4560.28\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.28e+03  |\n",
            "|    mean_reward     | -3.07e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.83e+03 |\n",
            "|    critic_loss     | 4.24e+03  |\n",
            "|    ent_coef        | 4.89      |\n",
            "|    ent_coef_loss   | -1.34     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 209699    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-440806.87 +/- 285975.98\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -4.41e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.73e+03 |\n",
            "|    critic_loss     | 8.28e+03  |\n",
            "|    ent_coef        | 4.75      |\n",
            "|    ent_coef_loss   | -1.27     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 219699    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-336332.32 +/- 164789.33\n",
            "Episode length: 8012.80 +/- 3976.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -3.36e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.97e+03 |\n",
            "|    critic_loss     | 1.28e+04  |\n",
            "|    ent_coef        | 4.7       |\n",
            "|    ent_coef_loss   | -1.98     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 229699    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-166299.45 +/- 179150.55\n",
            "Episode length: 6032.20 +/- 4860.77\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.03e+03  |\n",
            "|    mean_reward     | -1.66e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.72e+03 |\n",
            "|    critic_loss     | 5.5e+03   |\n",
            "|    ent_coef        | 5.2       |\n",
            "|    ent_coef_loss   | 0.556     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 239699    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-218733.87 +/- 169161.06\n",
            "Episode length: 6040.20 +/- 4851.02\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.04e+03  |\n",
            "|    mean_reward     | -2.19e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.15e+03 |\n",
            "|    critic_loss     | 6.71e+03  |\n",
            "|    ent_coef        | 5.36      |\n",
            "|    ent_coef_loss   | -0.304    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 249699    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-42179.79 +/- 36869.28\n",
            "Episode length: 4528.80 +/- 4554.05\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.53e+03  |\n",
            "|    mean_reward     | -4.22e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.72e+03 |\n",
            "|    critic_loss     | 7.09e+03  |\n",
            "|    ent_coef        | 5.19      |\n",
            "|    ent_coef_loss   | 1.77      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 259699    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=70000, episode_reward=-431558.65 +/- 370542.46\n",
            "Episode length: 8036.60 +/- 3928.80\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.04e+03  |\n",
            "|    mean_reward     | -4.32e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.95e+03 |\n",
            "|    critic_loss     | 5.14e+03  |\n",
            "|    ent_coef        | 5.28      |\n",
            "|    ent_coef_loss   | -1.52     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 269699    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 7.52e+03  |\n",
            "|    ep_rew_mean     | -3.85e+05 |\n",
            "| time/              |           |\n",
            "|    episodes        | 10        |\n",
            "|    fps             | 62        |\n",
            "|    time_elapsed    | 1203      |\n",
            "|    total_timesteps | 75187     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.9e+03  |\n",
            "|    critic_loss     | 4.57e+03  |\n",
            "|    ent_coef        | 4.73      |\n",
            "|    ent_coef_loss   | -0.246    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 274886    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-402161.13 +/- 242890.70\n",
            "Episode length: 8715.40 +/- 2571.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.72e+03  |\n",
            "|    mean_reward     | -4.02e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.54e+03 |\n",
            "|    critic_loss     | 1.25e+04  |\n",
            "|    ent_coef        | 4.63      |\n",
            "|    ent_coef_loss   | -0.372    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 279699    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-248154.36 +/- 351286.69\n",
            "Episode length: 4074.80 +/- 4838.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.07e+03  |\n",
            "|    mean_reward     | -2.48e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.77e+03 |\n",
            "|    critic_loss     | 7.64e+03  |\n",
            "|    ent_coef        | 4.39      |\n",
            "|    ent_coef_loss   | 0.406     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 289699    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-341506.27 +/- 331222.59\n",
            "Episode length: 8145.40 +/- 3711.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.15e+03  |\n",
            "|    mean_reward     | -3.42e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.53e+03 |\n",
            "|    critic_loss     | 3.66e+03  |\n",
            "|    ent_coef        | 4.51      |\n",
            "|    ent_coef_loss   | 0.993     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 299699    |\n",
            "----------------------------------\n",
            "mean reward: -461212.67481859995, std reward: +/-142333.11013925337\n",
            "Training itteration  3\n",
            "Eval num_timesteps=10000, episode_reward=-165217.39 +/- 215278.16\n",
            "Episode length: 4713.40 +/- 4480.25\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.71e+03  |\n",
            "|    mean_reward     | -1.65e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.33e+03 |\n",
            "|    critic_loss     | 5.95e+03  |\n",
            "|    ent_coef        | 4.23      |\n",
            "|    ent_coef_loss   | -1.61     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 309599    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-172236.16 +/- 164076.16\n",
            "Episode length: 8013.40 +/- 3975.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -1.72e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.38e+03 |\n",
            "|    critic_loss     | 5.5e+03   |\n",
            "|    ent_coef        | 4.24      |\n",
            "|    ent_coef_loss   | 1.22      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 319599    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-408555.69 +/- 285299.06\n",
            "Episode length: 8786.80 +/- 2428.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.79e+03  |\n",
            "|    mean_reward     | -4.09e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.44e+03 |\n",
            "|    critic_loss     | 1.99e+03  |\n",
            "|    ent_coef        | 4.03      |\n",
            "|    ent_coef_loss   | -1.03     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 329599    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-257509.64 +/- 184428.66\n",
            "Episode length: 7645.00 +/- 3854.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 7.64e+03  |\n",
            "|    mean_reward     | -2.58e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.22e+03 |\n",
            "|    critic_loss     | 2.51e+03  |\n",
            "|    ent_coef        | 3.77      |\n",
            "|    ent_coef_loss   | 0.725     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 339599    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-195236.63 +/- 236719.67\n",
            "Episode length: 4114.40 +/- 4808.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.11e+03  |\n",
            "|    mean_reward     | -1.95e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.11e+03 |\n",
            "|    critic_loss     | 3.6e+03   |\n",
            "|    ent_coef        | 3.73      |\n",
            "|    ent_coef_loss   | -0.787    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 349599    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-220396.11 +/- 246854.85\n",
            "Episode length: 6047.20 +/- 4842.41\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.05e+03  |\n",
            "|    mean_reward     | -2.2e+05  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.65e+03 |\n",
            "|    critic_loss     | 3.37e+03  |\n",
            "|    ent_coef        | 3.66      |\n",
            "|    ent_coef_loss   | -1.49     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 359599    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-481068.74 +/- 333267.10\n",
            "Episode length: 8055.80 +/- 3890.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.06e+03  |\n",
            "|    mean_reward     | -4.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.01e+03 |\n",
            "|    critic_loss     | 3.8e+03   |\n",
            "|    ent_coef        | 3.74      |\n",
            "|    ent_coef_loss   | 0.278     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 369599    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-160973.12 +/- 124931.14\n",
            "Episode length: 6309.80 +/- 4543.15\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.31e+03  |\n",
            "|    mean_reward     | -1.61e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.64e+03 |\n",
            "|    critic_loss     | 1.67e+04  |\n",
            "|    ent_coef        | 3.56      |\n",
            "|    ent_coef_loss   | 0.382     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 379599    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 8.24e+03  |\n",
            "|    ep_rew_mean     | -3.97e+05 |\n",
            "| time/              |           |\n",
            "|    episodes        | 10        |\n",
            "|    fps             | 63        |\n",
            "|    time_elapsed    | 1297      |\n",
            "|    total_timesteps | 82432     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.6e+03  |\n",
            "|    critic_loss     | 3.17e+03  |\n",
            "|    ent_coef        | 3.59      |\n",
            "|    ent_coef_loss   | -0.598    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 382031    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-373121.52 +/- 328243.69\n",
            "Episode length: 6043.00 +/- 4847.77\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.04e+03  |\n",
            "|    mean_reward     | -3.73e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.86e+03 |\n",
            "|    critic_loss     | 3.13e+03  |\n",
            "|    ent_coef        | 3.59      |\n",
            "|    ent_coef_loss   | -0.0248   |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 389599    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-257456.31 +/- 137879.82\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -2.57e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.48e+03 |\n",
            "|    critic_loss     | 3.94e+03  |\n",
            "|    ent_coef        | 3.57      |\n",
            "|    ent_coef_loss   | 0.186     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 399599    |\n",
            "----------------------------------\n",
            "mean reward: -233031.17436739997, std reward: +/-202952.95205826385\n",
            "Training itteration  4\n",
            "Eval num_timesteps=10000, episode_reward=-332642.49 +/- 274717.29\n",
            "Episode length: 8029.20 +/- 3943.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.03e+03  |\n",
            "|    mean_reward     | -3.33e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.23e+03 |\n",
            "|    critic_loss     | 1.52e+04  |\n",
            "|    ent_coef        | 3.5       |\n",
            "|    ent_coef_loss   | -0.948    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 409499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-353507.83 +/- 189470.66\n",
            "Episode length: 8013.20 +/- 3975.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -3.54e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.16e+03 |\n",
            "|    critic_loss     | 1.42e+03  |\n",
            "|    ent_coef        | 3.61      |\n",
            "|    ent_coef_loss   | 1.2       |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 419499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-285381.66 +/- 224282.58\n",
            "Episode length: 6411.40 +/- 4438.68\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.41e+03  |\n",
            "|    mean_reward     | -2.85e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.14e+03 |\n",
            "|    critic_loss     | 2.03e+03  |\n",
            "|    ent_coef        | 3.69      |\n",
            "|    ent_coef_loss   | 0.975     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 429499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-27020.10 +/- 14296.49\n",
            "Episode length: 2570.40 +/- 3840.19\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 2.57e+03  |\n",
            "|    mean_reward     | -2.7e+04  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.72e+03 |\n",
            "|    critic_loss     | 2.85e+03  |\n",
            "|    ent_coef        | 3.83      |\n",
            "|    ent_coef_loss   | -0.0576   |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 439499    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=50000, episode_reward=-316343.75 +/- 239101.90\n",
            "Episode length: 8014.80 +/- 3972.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -3.16e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.6e+03  |\n",
            "|    critic_loss     | 5.01e+03  |\n",
            "|    ent_coef        | 3.66      |\n",
            "|    ent_coef_loss   | 0.143     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 449499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-77020.03 +/- 55452.91\n",
            "Episode length: 6321.60 +/- 4513.65\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.32e+03  |\n",
            "|    mean_reward     | -7.7e+04  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.59e+03 |\n",
            "|    critic_loss     | 3.25e+03  |\n",
            "|    ent_coef        | 3.45      |\n",
            "|    ent_coef_loss   | 0.0194    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 459499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-293563.59 +/- 155534.42\n",
            "Episode length: 8076.40 +/- 3849.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.08e+03  |\n",
            "|    mean_reward     | -2.94e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.32e+03 |\n",
            "|    critic_loss     | 1.8e+04   |\n",
            "|    ent_coef        | 3.55      |\n",
            "|    ent_coef_loss   | -1.72     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 469499    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 7.3e+03   |\n",
            "|    ep_rew_mean     | -3.71e+05 |\n",
            "| time/              |           |\n",
            "|    episodes        | 10        |\n",
            "|    fps             | 63        |\n",
            "|    time_elapsed    | 1145      |\n",
            "|    total_timesteps | 73023     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.73e+03 |\n",
            "|    critic_loss     | 1.83e+03  |\n",
            "|    ent_coef        | 3.52      |\n",
            "|    ent_coef_loss   | 0.464     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 472522    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-287320.99 +/- 175693.35\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -2.87e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.56e+03 |\n",
            "|    critic_loss     | 4.65e+03  |\n",
            "|    ent_coef        | 3.41      |\n",
            "|    ent_coef_loss   | -0.248    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 479499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-314846.75 +/- 297345.75\n",
            "Episode length: 4624.80 +/- 4518.02\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.62e+03  |\n",
            "|    mean_reward     | -3.15e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.75e+03 |\n",
            "|    critic_loss     | 4.52e+03  |\n",
            "|    ent_coef        | 3.23      |\n",
            "|    ent_coef_loss   | 0.244     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 489499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-88737.97 +/- 68066.59\n",
            "Episode length: 4473.60 +/- 4580.97\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.47e+03  |\n",
            "|    mean_reward     | -8.87e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.46e+03 |\n",
            "|    critic_loss     | 3.16e+04  |\n",
            "|    ent_coef        | 3.12      |\n",
            "|    ent_coef_loss   | -0.315    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 499499    |\n",
            "----------------------------------\n",
            "mean reward: -183889.6894646, std reward: +/-213882.7775477537\n",
            "Training itteration  5\n",
            "Eval num_timesteps=10000, episode_reward=-411892.05 +/- 234498.92\n",
            "Episode length: 8363.20 +/- 3275.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.36e+03  |\n",
            "|    mean_reward     | -4.12e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.94e+03 |\n",
            "|    critic_loss     | 2.63e+03  |\n",
            "|    ent_coef        | 3.11      |\n",
            "|    ent_coef_loss   | -0.852    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 509399    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-280641.84 +/- 289154.42\n",
            "Episode length: 6054.60 +/- 4833.50\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.05e+03  |\n",
            "|    mean_reward     | -2.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.82e+03 |\n",
            "|    critic_loss     | 2.51e+03  |\n",
            "|    ent_coef        | 3.09      |\n",
            "|    ent_coef_loss   | 0.1       |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 519399    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-172238.04 +/- 131222.81\n",
            "Episode length: 6092.20 +/- 4788.14\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.09e+03  |\n",
            "|    mean_reward     | -1.72e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.34e+03 |\n",
            "|    critic_loss     | 2.27e+03  |\n",
            "|    ent_coef        | 2.97      |\n",
            "|    ent_coef_loss   | 0.0865    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 529399    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-158186.21 +/- 261537.80\n",
            "Episode length: 2413.20 +/- 3847.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 2.41e+03  |\n",
            "|    mean_reward     | -1.58e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.29e+03 |\n",
            "|    critic_loss     | 3.11e+03  |\n",
            "|    ent_coef        | 2.76      |\n",
            "|    ent_coef_loss   | -0.332    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 539399    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-551280.44 +/- 409564.70\n",
            "Episode length: 8061.20 +/- 3879.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.06e+03  |\n",
            "|    mean_reward     | -5.51e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.25e+03 |\n",
            "|    critic_loss     | 2.21e+03  |\n",
            "|    ent_coef        | 2.76      |\n",
            "|    ent_coef_loss   | -1.77     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 549399    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5.97e+03  |\n",
            "|    ep_rew_mean     | -1.7e+05  |\n",
            "| time/              |           |\n",
            "|    episodes        | 10        |\n",
            "|    fps             | 67        |\n",
            "|    time_elapsed    | 887       |\n",
            "|    total_timesteps | 59687     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.81e+03 |\n",
            "|    critic_loss     | 3.07e+03  |\n",
            "|    ent_coef        | 2.79      |\n",
            "|    ent_coef_loss   | -0.327    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 559086    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-270102.78 +/- 349029.16\n",
            "Episode length: 4135.60 +/- 4789.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 4.14e+03 |\n",
            "|    mean_reward     | -2.7e+05 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 60000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -2.3e+03 |\n",
            "|    critic_loss     | 5.68e+03 |\n",
            "|    ent_coef        | 2.79     |\n",
            "|    ent_coef_loss   | 0.0744   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 559399   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-403636.45 +/- 284237.57\n",
            "Episode length: 8015.20 +/- 3971.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.02e+03  |\n",
            "|    mean_reward     | -4.04e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.83e+03 |\n",
            "|    critic_loss     | 3.22e+03  |\n",
            "|    ent_coef        | 2.7       |\n",
            "|    ent_coef_loss   | -0.00616  |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 569399    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-90164.20 +/- 128058.79\n",
            "Episode length: 4049.20 +/- 4188.77\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.05e+03  |\n",
            "|    mean_reward     | -9.02e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.61e+03 |\n",
            "|    critic_loss     | 5.52e+03  |\n",
            "|    ent_coef        | 2.76      |\n",
            "|    ent_coef_loss   | -0.707    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 579399    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-197648.62 +/- 241523.13\n",
            "Episode length: 6031.00 +/- 4862.24\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.03e+03  |\n",
            "|    mean_reward     | -1.98e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.1e+03  |\n",
            "|    critic_loss     | 5.16e+03  |\n",
            "|    ent_coef        | 2.66      |\n",
            "|    ent_coef_loss   | -0.352    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 589399    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-401486.60 +/- 223780.97\n",
            "Episode length: 8056.80 +/- 3888.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.06e+03  |\n",
            "|    mean_reward     | -4.01e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.45e+03 |\n",
            "|    critic_loss     | 2.03e+03  |\n",
            "|    ent_coef        | 2.71      |\n",
            "|    ent_coef_loss   | 0.887     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 599399    |\n",
            "----------------------------------\n",
            "mean reward: -315453.44740420004, std reward: +/-272504.3185893298\n",
            "Training itteration  6\n",
            "Eval num_timesteps=10000, episode_reward=-132135.26 +/- 171072.24\n",
            "Episode length: 4163.80 +/- 4769.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.16e+03  |\n",
            "|    mean_reward     | -1.32e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.97e+03 |\n",
            "|    critic_loss     | 3.35e+03  |\n",
            "|    ent_coef        | 2.88      |\n",
            "|    ent_coef_loss   | 0.0596    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 609299    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-409678.33 +/- 218707.11\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -4.1e+05  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.55e+03 |\n",
            "|    critic_loss     | 2.25e+03  |\n",
            "|    ent_coef        | 2.9       |\n",
            "|    ent_coef_loss   | 1.77      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 619299    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-291291.55 +/- 261139.09\n",
            "Episode length: 6106.80 +/- 4770.89\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.11e+03  |\n",
            "|    mean_reward     | -2.91e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.94e+03 |\n",
            "|    critic_loss     | 4.88e+03  |\n",
            "|    ent_coef        | 2.77      |\n",
            "|    ent_coef_loss   | -0.768    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 629299    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-313657.89 +/- 222474.83\n",
            "Episode length: 8008.20 +/- 3985.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -3.14e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.57e+03 |\n",
            "|    critic_loss     | 2.17e+03  |\n",
            "|    ent_coef        | 2.8       |\n",
            "|    ent_coef_loss   | 0.0932    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 639299    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-421908.11 +/- 353462.77\n",
            "Episode length: 6130.40 +/- 4743.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.13e+03  |\n",
            "|    mean_reward     | -4.22e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.69e+03 |\n",
            "|    critic_loss     | 3.05e+03  |\n",
            "|    ent_coef        | 2.77      |\n",
            "|    ent_coef_loss   | -0.55     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 649299    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5.46e+03  |\n",
            "|    ep_rew_mean     | -3.45e+05 |\n",
            "| time/              |           |\n",
            "|    episodes        | 10        |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 852       |\n",
            "|    total_timesteps | 54559     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.72e+03 |\n",
            "|    critic_loss     | 5.96e+03  |\n",
            "|    ent_coef        | 2.73      |\n",
            "|    ent_coef_loss   | 0.178     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 653858    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-270100.42 +/- 229202.00\n",
            "Episode length: 8481.00 +/- 2488.06\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.48e+03  |\n",
            "|    mean_reward     | -2.7e+05  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.53e+03 |\n",
            "|    critic_loss     | 2.07e+03  |\n",
            "|    ent_coef        | 2.75      |\n",
            "|    ent_coef_loss   | 0.2       |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 659299    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-207555.34 +/- 143500.43\n",
            "Episode length: 8303.80 +/- 3394.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.3e+03   |\n",
            "|    mean_reward     | -2.08e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.06e+03 |\n",
            "|    critic_loss     | 2.75e+03  |\n",
            "|    ent_coef        | 2.76      |\n",
            "|    ent_coef_loss   | 0.769     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 669299    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-239820.14 +/- 114099.26\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -2.4e+05  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.56e+03 |\n",
            "|    critic_loss     | 3.31e+03  |\n",
            "|    ent_coef        | 2.81      |\n",
            "|    ent_coef_loss   | 0.755     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 679299    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-515174.57 +/- 476763.76\n",
            "Episode length: 8045.80 +/- 3910.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.05e+03  |\n",
            "|    mean_reward     | -5.15e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.01e+03 |\n",
            "|    critic_loss     | 3.23e+03  |\n",
            "|    ent_coef        | 2.7       |\n",
            "|    ent_coef_loss   | -0.818    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 689299    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-255063.06 +/- 130224.61\n",
            "Episode length: 8044.20 +/- 3913.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.04e+03  |\n",
            "|    mean_reward     | -2.55e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.71e+03 |\n",
            "|    critic_loss     | 4.83e+03  |\n",
            "|    ent_coef        | 2.66      |\n",
            "|    ent_coef_loss   | 0.922     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 699299    |\n",
            "----------------------------------\n",
            "mean reward: -484532.84436839994, std reward: +/-336469.71142596676\n",
            "Training itteration  7\n",
            "Eval num_timesteps=10000, episode_reward=-370671.42 +/- 213418.45\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -3.71e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.98e+03 |\n",
            "|    critic_loss     | 1.17e+04  |\n",
            "|    ent_coef        | 2.63      |\n",
            "|    ent_coef_loss   | 0.539     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 709199    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-246953.52 +/- 211562.75\n",
            "Episode length: 8100.40 +/- 3801.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.1e+03   |\n",
            "|    mean_reward     | -2.47e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.05e+03 |\n",
            "|    critic_loss     | 4.27e+03  |\n",
            "|    ent_coef        | 2.57      |\n",
            "|    ent_coef_loss   | -0.766    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 719199    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-324831.96 +/- 316138.19\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -3.25e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.07e+03 |\n",
            "|    critic_loss     | 3.17e+03  |\n",
            "|    ent_coef        | 2.43      |\n",
            "|    ent_coef_loss   | 0.707     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 729199    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-336634.25 +/- 305650.68\n",
            "Episode length: 6097.60 +/- 4782.58\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.1e+03   |\n",
            "|    mean_reward     | -3.37e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.69e+03 |\n",
            "|    critic_loss     | 3.58e+03  |\n",
            "|    ent_coef        | 2.53      |\n",
            "|    ent_coef_loss   | -0.703    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 739199    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-402010.55 +/- 275678.36\n",
            "Episode length: 8016.40 +/- 3969.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.02e+03  |\n",
            "|    mean_reward     | -4.02e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -803      |\n",
            "|    critic_loss     | 1.85e+03  |\n",
            "|    ent_coef        | 2.38      |\n",
            "|    ent_coef_loss   | 0.572     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 749199    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5.81e+03  |\n",
            "|    ep_rew_mean     | -2.1e+05  |\n",
            "| time/              |           |\n",
            "|    episodes        | 10        |\n",
            "|    fps             | 62        |\n",
            "|    time_elapsed    | 936       |\n",
            "|    total_timesteps | 58115     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.49e+03 |\n",
            "|    critic_loss     | 5.3e+03   |\n",
            "|    ent_coef        | 2.56      |\n",
            "|    ent_coef_loss   | 0.446     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 757314    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-301105.02 +/- 321448.02\n",
            "Episode length: 8014.20 +/- 3973.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -3.01e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.04e+03 |\n",
            "|    critic_loss     | 2.81e+03  |\n",
            "|    ent_coef        | 2.55      |\n",
            "|    ent_coef_loss   | 0.748     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 759199    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-324674.38 +/- 383864.15\n",
            "Episode length: 6028.20 +/- 4865.69\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.03e+03  |\n",
            "|    mean_reward     | -3.25e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.47e+03 |\n",
            "|    critic_loss     | 3.76e+03  |\n",
            "|    ent_coef        | 3.04      |\n",
            "|    ent_coef_loss   | 0.214     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 769199    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-352136.11 +/- 310093.83\n",
            "Episode length: 6101.80 +/- 4777.08\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.1e+03   |\n",
            "|    mean_reward     | -3.52e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.28e+03 |\n",
            "|    critic_loss     | 2.73e+03  |\n",
            "|    ent_coef        | 2.67      |\n",
            "|    ent_coef_loss   | 0.878     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 779199    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-341372.63 +/- 324766.94\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -3.41e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.86e+03 |\n",
            "|    critic_loss     | 4.12e+03  |\n",
            "|    ent_coef        | 2.47      |\n",
            "|    ent_coef_loss   | 0.0178    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 789199    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-398934.10 +/- 211288.20\n",
            "Episode length: 8026.00 +/- 3950.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.03e+03  |\n",
            "|    mean_reward     | -3.99e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.09e+03 |\n",
            "|    critic_loss     | 2.08e+03  |\n",
            "|    ent_coef        | 2.38      |\n",
            "|    ent_coef_loss   | 0.716     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 799199    |\n",
            "----------------------------------\n",
            "mean reward: -354715.2165996, std reward: +/-212676.133950789\n",
            "Training itteration  8\n",
            "Eval num_timesteps=10000, episode_reward=-591525.40 +/- 330588.07\n",
            "Episode length: 8014.60 +/- 3972.80\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -5.92e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -801      |\n",
            "|    critic_loss     | 2.09e+06  |\n",
            "|    ent_coef        | 2.38      |\n",
            "|    ent_coef_loss   | 0.861     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 809099    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-407512.00 +/- 321015.69\n",
            "Episode length: 8013.60 +/- 3974.80\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -4.08e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -306      |\n",
            "|    critic_loss     | 6.84e+03  |\n",
            "|    ent_coef        | 2.27      |\n",
            "|    ent_coef_loss   | -0.189    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 819099    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-204617.86 +/- 213684.17\n",
            "Episode length: 6060.80 +/- 4825.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.06e+03  |\n",
            "|    mean_reward     | -2.05e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -617      |\n",
            "|    critic_loss     | 2.67e+03  |\n",
            "|    ent_coef        | 2.31      |\n",
            "|    ent_coef_loss   | -0.404    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 829099    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-211979.65 +/- 248213.06\n",
            "Episode length: 4192.40 +/- 4745.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.19e+03  |\n",
            "|    mean_reward     | -2.12e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.3e+03  |\n",
            "|    critic_loss     | 8.65e+03  |\n",
            "|    ent_coef        | 2.32      |\n",
            "|    ent_coef_loss   | -1.32     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 839099    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-293772.16 +/- 234212.66\n",
            "Episode length: 6051.40 +/- 4837.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.05e+03  |\n",
            "|    mean_reward     | -2.94e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -256      |\n",
            "|    critic_loss     | 3.96e+03  |\n",
            "|    ent_coef        | 2.48      |\n",
            "|    ent_coef_loss   | 0.544     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 849099    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-494761.37 +/- 252384.08\n",
            "Episode length: 8577.40 +/- 2847.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.58e+03  |\n",
            "|    mean_reward     | -4.95e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 672       |\n",
            "|    critic_loss     | 1.88e+03  |\n",
            "|    ent_coef        | 2.45      |\n",
            "|    ent_coef_loss   | 0.519     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 859099    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-113594.02 +/- 80139.55\n",
            "Episode length: 6167.80 +/- 4698.71\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.17e+03  |\n",
            "|    mean_reward     | -1.14e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 105       |\n",
            "|    critic_loss     | 1.27e+03  |\n",
            "|    ent_coef        | 2.46      |\n",
            "|    ent_coef_loss   | 0.143     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 869099    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-298939.89 +/- 204400.25\n",
            "Episode length: 8109.40 +/- 3783.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.11e+03  |\n",
            "|    mean_reward     | -2.99e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -604      |\n",
            "|    critic_loss     | 5e+03     |\n",
            "|    ent_coef        | 2.52      |\n",
            "|    ent_coef_loss   | -0.0875   |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 879099    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 8.28e+03  |\n",
            "|    ep_rew_mean     | -4.82e+05 |\n",
            "| time/              |           |\n",
            "|    episodes        | 10        |\n",
            "|    fps             | 63        |\n",
            "|    time_elapsed    | 1306      |\n",
            "|    total_timesteps | 82817     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -819      |\n",
            "|    critic_loss     | 2.94e+03  |\n",
            "|    ent_coef        | 2.48      |\n",
            "|    ent_coef_loss   | 0.112     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 881916    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-240206.35 +/- 169521.71\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -2.4e+05  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.53e+03 |\n",
            "|    critic_loss     | 4.16e+03  |\n",
            "|    ent_coef        | 2.47      |\n",
            "|    ent_coef_loss   | -0.596    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 889099    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-193829.11 +/- 162315.24\n",
            "Episode length: 6034.40 +/- 4858.08\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.03e+03  |\n",
            "|    mean_reward     | -1.94e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -308      |\n",
            "|    critic_loss     | 2.79e+03  |\n",
            "|    ent_coef        | 2.46      |\n",
            "|    ent_coef_loss   | 0.341     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 899099    |\n",
            "----------------------------------\n",
            "mean reward: -236903.90708100004, std reward: +/-209248.8194565837\n",
            "Training itteration  9\n",
            "Eval num_timesteps=10000, episode_reward=-305359.61 +/- 252621.65\n",
            "Episode length: 6045.60 +/- 4844.36\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.05e+03  |\n",
            "|    mean_reward     | -3.05e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.07e+03 |\n",
            "|    critic_loss     | 2.18e+03  |\n",
            "|    ent_coef        | 2.31      |\n",
            "|    ent_coef_loss   | 0.247     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 908999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-481716.26 +/- 116486.44\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -4.82e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -207      |\n",
            "|    critic_loss     | 1.8e+03   |\n",
            "|    ent_coef        | 2.37      |\n",
            "|    ent_coef_loss   | -0.618    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 918999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-210809.75 +/- 201641.07\n",
            "Episode length: 4405.40 +/- 4619.58\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 4.41e+03  |\n",
            "|    mean_reward     | -2.11e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -179      |\n",
            "|    critic_loss     | 2.92e+03  |\n",
            "|    ent_coef        | 2.34      |\n",
            "|    ent_coef_loss   | -0.749    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 928999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-181428.19 +/- 194125.74\n",
            "Episode length: 6072.20 +/- 4812.37\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.07e+03  |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 51.3      |\n",
            "|    critic_loss     | 1.27e+04  |\n",
            "|    ent_coef        | 2.29      |\n",
            "|    ent_coef_loss   | 0.647     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 938999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-343469.17 +/- 268409.52\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -3.43e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.03e+03 |\n",
            "|    critic_loss     | 2.84e+03  |\n",
            "|    ent_coef        | 2.39      |\n",
            "|    ent_coef_loss   | 1.03      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 948999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-318370.12 +/- 231761.30\n",
            "Episode length: 8016.40 +/- 3969.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.02e+03  |\n",
            "|    mean_reward     | -3.18e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -862      |\n",
            "|    critic_loss     | 1.12e+04  |\n",
            "|    ent_coef        | 2.34      |\n",
            "|    ent_coef_loss   | -1.01     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 958999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-335512.07 +/- 267833.61\n",
            "Episode length: 6098.40 +/- 4781.02\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.1e+03   |\n",
            "|    mean_reward     | -3.36e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.25e+03 |\n",
            "|    critic_loss     | 3.11e+03  |\n",
            "|    ent_coef        | 2.35      |\n",
            "|    ent_coef_loss   | -1.43     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 968999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-272689.36 +/- 311883.37\n",
            "Episode length: 6068.20 +/- 4816.85\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 6.07e+03  |\n",
            "|    mean_reward     | -2.73e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -468      |\n",
            "|    critic_loss     | 2.91e+03  |\n",
            "|    ent_coef        | 2.46      |\n",
            "|    ent_coef_loss   | -0.184    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 978999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-500695.53 +/- 60122.47\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+04     |\n",
            "|    mean_reward     | -5.01e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.56e+03 |\n",
            "|    critic_loss     | 3.03e+03  |\n",
            "|    ent_coef        | 2.57      |\n",
            "|    ent_coef_loss   | -0.521    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 988999    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-348428.76 +/- 225048.48\n",
            "Episode length: 8014.60 +/- 3972.80\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 8.01e+03  |\n",
            "|    mean_reward     | -3.48e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.09e+03 |\n",
            "|    critic_loss     | 4.28e+03  |\n",
            "|    ent_coef        | 2.71      |\n",
            "|    ent_coef_loss   | -0.322    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 998999    |\n",
            "----------------------------------\n",
            "mean reward: -445485.02461719996, std reward: +/-314590.6736755695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(mean_rewards)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title('Mean Reward vs Iteration')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9hPQ2yZMANTN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "21c0be62-61bc-41a0-faf1-3cecdc63f0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}