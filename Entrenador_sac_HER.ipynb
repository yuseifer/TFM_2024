{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuseifer/TFM_2024/blob/main/Entrenador_sac_HER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3EUxIs-49Uo",
        "outputId": "9442f7ec-240f-453f-8f05-2d299cc270f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.26.4)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654616 sha256=d1ddb275d91b29bdbe0a66dfd06b01387e21d4cda0d2d7161e1d97530a04f937\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.0\n",
            "    Uninstalling cloudpickle-3.1.0:\n",
            "      Successfully uninstalled cloudpickle-3.1.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.25.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "dask 2024.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n",
            "Collecting pybullet==3.2.6\n",
            "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.6\n",
            "Collecting stable_baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable_baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (2.5.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (2.17.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (4.66.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (13.9.3)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable_baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable_baselines3[extra]) (10.4.0)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (2.32.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable_baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable_baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.0.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable_baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable_baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable_baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable_baselines3[extra]) (6.4.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable_baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable_baselines3[extra]) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable_baselines3[extra]) (2024.8.30)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=7c90505205c48cc88ca70abef4fadfb6c1ba385cfbea0e6db443d9771a769f0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable_baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-1.3.0 stable_baselines3-2.3.2\n",
            "Collecting shimmy==1.2.1\n",
            "  Downloading Shimmy-1.2.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy==1.2.1) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy==1.2.1) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy==1.2.1) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy==1.2.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy==1.2.1) (0.0.4)\n",
            "Downloading Shimmy-1.2.1-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: shimmy\n",
            "  Attempting uninstall: shimmy\n",
            "    Found existing installation: Shimmy 1.3.0\n",
            "    Uninstalling Shimmy-1.3.0:\n",
            "      Successfully uninstalled Shimmy-1.3.0\n",
            "Successfully installed shimmy-1.2.1\n",
            "Collecting gymnasium==0.28.1\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (1.26.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (0.0.4)\n",
            "Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: jax-jumpy, gymnasium\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 0.29.1\n",
            "    Uninstalling gymnasium-0.29.1:\n",
            "      Successfully uninstalled gymnasium-0.29.1\n",
            "Successfully installed gymnasium-0.28.1 jax-jumpy-1.0.0\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.17.3\n",
        "!pip install pybullet==3.2.6\n",
        "!pip install stable_baselines3[extra]\n",
        "!pip install shimmy==1.2.1\n",
        "!pip install gymnasium==0.28.1\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhyzKzwB5G0x",
        "outputId": "ec959efb-74e2-480b-ec72-6428ae604f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ant_custom.zip\n",
            "   creating: ant_custom/\n",
            "   creating: ant_custom/CustomAnt_Env/\n",
            "   creating: ant_custom/CustomAnt_Env/envs/\n",
            "  inflating: ant_custom/CustomAnt_Env/envs/AntCustomEnvEmpty.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/envs/__init__.py  \n",
            "   creating: ant_custom/CustomAnt_Env/resources/\n",
            "  inflating: ant_custom/CustomAnt_Env/resources/antcustom.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/Cil_obs.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/goal.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/plane.py  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/simplegoal.urdf  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/simpleobstacle.urdf  \n",
            "  inflating: ant_custom/CustomAnt_Env/resources/simpleplane.urdf  \n",
            "  inflating: ant_custom/CustomAnt_Env/__init__.py  \n",
            "  inflating: ant_custom/setup.py     \n"
          ]
        }
      ],
      "source": [
        "!unzip ant_custom.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwjU16X25QTK"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import pybullet, pybullet_envs\n",
        "import torch as th\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "#Importamos librerías para realizar el multiprocesos y normalización del entorno\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "import gym\n",
        "import sys\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "sys.path.append('/content/ant_custom')\n",
        "import CustomAnt_Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2KADT6r5T1P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#We begin with training but first we set some hyperparameters\n",
        "SEED                = 42\n",
        "NUM_ENVS            = 8\n",
        "ENV_ID              = 'AntCustomEnv-v0'\n",
        "HIDDEN_SIZE         = 256\n",
        "LEARNING_RATE       = 1e-4\n",
        "GAMMA               = 0.99\n",
        "GAE_LAMBDA          = 0.95\n",
        "PPO_EPSILON         = 0.2\n",
        "CRITIC_DISCOUNT     = 0.5\n",
        "ENTROPY_BETA        = 0.001\n",
        "PPO_STEPS           = 256\n",
        "MINI_BATCH_SIZE     = 64\n",
        "PPO_EPOCHS          = 10\n",
        "TEST_EPOCHS         = 10\n",
        "NUM_TESTS           = 10\n",
        "TARGET_REWARD       = 50\n",
        "MAX_STEPS           = 10000\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEPL6hsh5VrI"
      },
      "outputs": [],
      "source": [
        "# Create the env\n",
        "env = gym.make(ENV_ID)\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape\n",
        "\n",
        "a_size = env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZo2Niud5X5z"
      },
      "outputs": [],
      "source": [
        "MAX_AVERAGE_SCORE = 50e6\n",
        "#Definimos la arquitectura de la red\n",
        "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU, net_arch=[512, 512])\n",
        "#policy_kwargs = dict(activation_fn=th.nn.Tanh, net_arch=[512, 512, 264])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLL_bzAX5hHZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluar_modelo(model, num_tests, env):\n",
        "    rewards = []\n",
        "    for i in range(num_tests):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    mean_reward = np.mean(rewards)\n",
        "    std_reward = np.std(rewards)\n",
        "    return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UIoS_hz5iA7",
        "outputId": "33d53348-2842-4d96-89ea-e0b7c32c87a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "# Crear el callback de evaluación, evaluando cada 10,000 steps y guardando el mejor modelo\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model/',\n",
        "                             log_path='./logs/eval/', eval_freq=10000,\n",
        "                             deterministic=True, render=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfsR5x-AEA4B",
        "outputId": "9a4a73f3-2fa6-499a-d172-40af902fc63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Model created\n",
            "Training itteration  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=7600, episode_reward=-859.51 +/- 1996.22\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -860        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 7600        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012838498 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.2       |\n",
            "|    explained_variance   | 0.35        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.28        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0177     |\n",
            "|    std                  | 0.98        |\n",
            "|    value_loss           | 12.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=17600, episode_reward=27.12 +/- 2093.75\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | 27.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 17600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020435195 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.9       |\n",
            "|    explained_variance   | 0.306       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.26        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0117     |\n",
            "|    std                  | 0.947       |\n",
            "|    value_loss           | 7.31        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 114        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 178        |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01954136 |\n",
            "|    clip_fraction        | 0.188      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -10.9      |\n",
            "|    explained_variance   | 0.186      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.91       |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | -0.0115    |\n",
            "|    std                  | 0.944      |\n",
            "|    value_loss           | 6.8        |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=27600, episode_reward=-1721.89 +/- 1135.56\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | -1.72e+03  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 27600      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01615939 |\n",
            "|    clip_fraction        | 0.264      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -10.7      |\n",
            "|    explained_variance   | 0.587      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.39       |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.0122    |\n",
            "|    std                  | 0.918      |\n",
            "|    value_loss           | 5.26       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=37600, episode_reward=-798.36 +/- 1508.74\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -798        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 37600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019966044 |\n",
            "|    clip_fraction        | 0.281       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.4       |\n",
            "|    explained_variance   | 0.761       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.38        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0133     |\n",
            "|    std                  | 0.882       |\n",
            "|    value_loss           | 3.21        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 114         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 357         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015653364 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.3       |\n",
            "|    explained_variance   | 0.623       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.74        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00481    |\n",
            "|    std                  | 0.877       |\n",
            "|    value_loss           | 5.66        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=47600, episode_reward=-95.74 +/- 1606.83\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -95.7       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 47600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014710156 |\n",
            "|    clip_fraction        | 0.229       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.2       |\n",
            "|    explained_variance   | 0.396       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.35        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00969    |\n",
            "|    std                  | 0.859       |\n",
            "|    value_loss           | 5.25        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=57600, episode_reward=-980.37 +/- 1312.58\n",
            "Episode length: 8001.00 +/- 4000.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 8e+03      |\n",
            "|    mean_reward          | -980       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 57600      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01752651 |\n",
            "|    clip_fraction        | 0.286      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -9.96      |\n",
            "|    explained_variance   | 0.501      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.59       |\n",
            "|    n_updates            | 280        |\n",
            "|    policy_gradient_loss | -0.015     |\n",
            "|    std                  | 0.838      |\n",
            "|    value_loss           | 3.66       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 117         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 524         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019235156 |\n",
            "|    clip_fraction        | 0.281       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.92       |\n",
            "|    explained_variance   | 0.827       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.69        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0174     |\n",
            "|    std                  | 0.836       |\n",
            "|    value_loss           | 4.52        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=67600, episode_reward=-1076.70 +/- 1510.50\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | -1.08e+03  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 67600      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01861021 |\n",
            "|    clip_fraction        | 0.302      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -9.61      |\n",
            "|    explained_variance   | 0.195      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.34       |\n",
            "|    n_updates            | 330        |\n",
            "|    policy_gradient_loss | -0.0192    |\n",
            "|    std                  | 0.803      |\n",
            "|    value_loss           | 3.12       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=77600, episode_reward=107.60 +/- 763.45\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | 108         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 77600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028696042 |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.4        |\n",
            "|    explained_variance   | 0.418       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.74        |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.0133     |\n",
            "|    std                  | 0.781       |\n",
            "|    value_loss           | 3.93        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 116         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 703         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031470772 |\n",
            "|    clip_fraction        | 0.386       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.24       |\n",
            "|    explained_variance   | 0.674       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.984       |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.00588    |\n",
            "|    std                  | 0.766       |\n",
            "|    value_loss           | 3.05        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=87600, episode_reward=692.12 +/- 668.00\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | 692         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 87600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018766467 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.04       |\n",
            "|    explained_variance   | 0.723       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.42        |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.00413    |\n",
            "|    std                  | 0.747       |\n",
            "|    value_loss           | 3.63        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=97600, episode_reward=-1275.83 +/- 1049.04\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.28e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 97600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021741804 |\n",
            "|    clip_fraction        | 0.33        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.58       |\n",
            "|    explained_variance   | 0.872       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.34        |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.0184     |\n",
            "|    std                  | 0.705       |\n",
            "|    value_loss           | 1.92        |\n",
            "-----------------------------------------\n",
            "mean reward: -417.4735058, std reward: +/-1093.7221872315388\n",
            "Training itteration  1\n",
            "Eval num_timesteps=7248, episode_reward=-2463.41 +/- 4009.67\n",
            "Episode length: 8985.00 +/- 2032.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.98e+03    |\n",
            "|    mean_reward          | -2.46e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 7248        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027604893 |\n",
            "|    clip_fraction        | 0.303       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.28       |\n",
            "|    explained_variance   | 0.597       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.743       |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.0102     |\n",
            "|    std                  | 0.679       |\n",
            "|    value_loss           | 1.9         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=17248, episode_reward=-1397.09 +/- 1412.69\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.4e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 17248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023930378 |\n",
            "|    clip_fraction        | 0.269       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.12       |\n",
            "|    explained_variance   | 0.76        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.904       |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | 0.00232     |\n",
            "|    std                  | 0.665       |\n",
            "|    value_loss           | 3.07        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 118         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 172         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022081533 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.07       |\n",
            "|    explained_variance   | 0.725       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.724       |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.00703    |\n",
            "|    std                  | 0.663       |\n",
            "|    value_loss           | 3.05        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=27248, episode_reward=-1038.01 +/- 1433.52\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.04e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 27248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027248602 |\n",
            "|    clip_fraction        | 0.286       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -7.75       |\n",
            "|    explained_variance   | 0.939       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.711       |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    std                  | 0.636       |\n",
            "|    value_loss           | 1.45        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=37248, episode_reward=-2418.27 +/- 427.14\n",
            "Episode length: 9654.40 +/- 693.20\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 9.65e+03    |\n",
            "|    mean_reward          | -2.42e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 37248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039098687 |\n",
            "|    clip_fraction        | 0.336       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -7.38       |\n",
            "|    explained_variance   | 0.406       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.08        |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.00249    |\n",
            "|    std                  | 0.608       |\n",
            "|    value_loss           | 2.9         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 116         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 352         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.049463347 |\n",
            "|    clip_fraction        | 0.45        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -7.34       |\n",
            "|    explained_variance   | -2.28       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.857       |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | 0.00632     |\n",
            "|    std                  | 0.605       |\n",
            "|    value_loss           | 2.55        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=47248, episode_reward=-950.12 +/- 1454.12\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | -950       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 47248      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02596651 |\n",
            "|    clip_fraction        | 0.347      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -7.14      |\n",
            "|    explained_variance   | 0.515      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.05       |\n",
            "|    n_updates            | 720        |\n",
            "|    policy_gradient_loss | -0.00857   |\n",
            "|    std                  | 0.59       |\n",
            "|    value_loss           | 2.9        |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=57248, episode_reward=-1346.78 +/- 1145.23\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.35e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 57248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.047086876 |\n",
            "|    clip_fraction        | 0.426       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.98       |\n",
            "|    explained_variance   | 0.602       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.681       |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | 0.00367     |\n",
            "|    std                  | 0.577       |\n",
            "|    value_loss           | 1.46        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 114         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 535         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034169257 |\n",
            "|    clip_fraction        | 0.381       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.89       |\n",
            "|    explained_variance   | 0.556       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.483       |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | 0.0118      |\n",
            "|    std                  | 0.573       |\n",
            "|    value_loss           | 1.4         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=67248, episode_reward=20.66 +/- 1686.05\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | 20.7        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 67248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022131177 |\n",
            "|    clip_fraction        | 0.307       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.76       |\n",
            "|    explained_variance   | 0.259       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.15        |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    std                  | 0.563       |\n",
            "|    value_loss           | 1.6         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=77248, episode_reward=-485.41 +/- 2008.41\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -485        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 77248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031609587 |\n",
            "|    clip_fraction        | 0.323       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.56       |\n",
            "|    explained_variance   | 0.771       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.794       |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.00286    |\n",
            "|    std                  | 0.55        |\n",
            "|    value_loss           | 2.34        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 114         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 717         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034232706 |\n",
            "|    clip_fraction        | 0.346       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.47       |\n",
            "|    explained_variance   | 0.705       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.658       |\n",
            "|    n_updates            | 880         |\n",
            "|    policy_gradient_loss | -0.00557    |\n",
            "|    std                  | 0.543       |\n",
            "|    value_loss           | 1.61        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=87248, episode_reward=-1627.62 +/- 743.45\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.63e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 87248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024606943 |\n",
            "|    clip_fraction        | 0.296       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.3        |\n",
            "|    explained_variance   | 0.445       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.56        |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | -0.012      |\n",
            "|    std                  | 0.533       |\n",
            "|    value_loss           | 7.29        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=97248, episode_reward=-498.97 +/- 1392.40\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -499        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 97248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024301572 |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.04       |\n",
            "|    explained_variance   | 0.665       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.395       |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | -0.000424   |\n",
            "|    std                  | 0.517       |\n",
            "|    value_loss           | 1.67        |\n",
            "-----------------------------------------\n",
            "mean reward: -1173.4107960000001, std reward: +/-1461.7942292023743\n",
            "Training itteration  2\n",
            "Eval num_timesteps=6896, episode_reward=-1251.14 +/- 1393.17\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | -1.25e+03  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 6896       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03476591 |\n",
            "|    clip_fraction        | 0.368      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.88      |\n",
            "|    explained_variance   | 0.512      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.429      |\n",
            "|    n_updates            | 1010       |\n",
            "|    policy_gradient_loss | -0.00606   |\n",
            "|    std                  | 0.508      |\n",
            "|    value_loss           | 1.23       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=16896, episode_reward=-1572.29 +/- 1627.24\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.57e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16896       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022379203 |\n",
            "|    clip_fraction        | 0.347       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.39       |\n",
            "|    explained_variance   | 0.197       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.271       |\n",
            "|    n_updates            | 1060        |\n",
            "|    policy_gradient_loss | -0.00485    |\n",
            "|    std                  | 0.479       |\n",
            "|    value_loss           | 0.65        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 115        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 176        |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03988398 |\n",
            "|    clip_fraction        | 0.403      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.33      |\n",
            "|    explained_variance   | 0.825      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.475      |\n",
            "|    n_updates            | 1070       |\n",
            "|    policy_gradient_loss | 0.0158     |\n",
            "|    std                  | 0.478      |\n",
            "|    value_loss           | 2.77       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=26896, episode_reward=-809.93 +/- 1775.75\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | -810       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 26896      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02367333 |\n",
            "|    clip_fraction        | 0.301      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.06      |\n",
            "|    explained_variance   | 0.439      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.182      |\n",
            "|    n_updates            | 1110       |\n",
            "|    policy_gradient_loss | -0.00285   |\n",
            "|    std                  | 0.463      |\n",
            "|    value_loss           | 0.763      |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=36896, episode_reward=-1360.55 +/- 714.92\n",
            "Episode length: 8001.00 +/- 4000.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8e+03       |\n",
            "|    mean_reward          | -1.36e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 36896       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027101504 |\n",
            "|    clip_fraction        | 0.373       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.89       |\n",
            "|    explained_variance   | 0.245       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.459       |\n",
            "|    n_updates            | 1160        |\n",
            "|    policy_gradient_loss | -0.00273    |\n",
            "|    std                  | 0.454       |\n",
            "|    value_loss           | 0.81        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 341         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034604266 |\n",
            "|    clip_fraction        | 0.437       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.84       |\n",
            "|    explained_variance   | 0.153       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.669       |\n",
            "|    n_updates            | 1170        |\n",
            "|    policy_gradient_loss | 0.00987     |\n",
            "|    std                  | 0.452       |\n",
            "|    value_loss           | 1.87        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=46896, episode_reward=-1324.40 +/- 1124.07\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.32e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 46896       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030829042 |\n",
            "|    clip_fraction        | 0.341       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.59       |\n",
            "|    explained_variance   | 0.305       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.5         |\n",
            "|    n_updates            | 1200        |\n",
            "|    policy_gradient_loss | 0.000418    |\n",
            "|    std                  | 0.438       |\n",
            "|    value_loss           | 1.16        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=56896, episode_reward=10.66 +/- 1182.81\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | 10.7       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 56896      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01651388 |\n",
            "|    clip_fraction        | 0.238      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.28      |\n",
            "|    explained_variance   | 0.102      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.255      |\n",
            "|    n_updates            | 1250       |\n",
            "|    policy_gradient_loss | 0.00275    |\n",
            "|    std                  | 0.427      |\n",
            "|    value_loss           | 0.439      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 118         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 519         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036270957 |\n",
            "|    clip_fraction        | 0.437       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.21       |\n",
            "|    explained_variance   | 0.252       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.387       |\n",
            "|    n_updates            | 1270        |\n",
            "|    policy_gradient_loss | -0.00315    |\n",
            "|    std                  | 0.423       |\n",
            "|    value_loss           | 0.901       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=66896, episode_reward=220.95 +/- 1103.76\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | 221         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 66896       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029843658 |\n",
            "|    clip_fraction        | 0.346       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.01       |\n",
            "|    explained_variance   | 0.773       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.37        |\n",
            "|    n_updates            | 1300        |\n",
            "|    policy_gradient_loss | 0.00226     |\n",
            "|    std                  | 0.416       |\n",
            "|    value_loss           | 1.19        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=76896, episode_reward=-937.56 +/- 1913.21\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -938        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 76896       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033115767 |\n",
            "|    clip_fraction        | 0.349       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.77       |\n",
            "|    explained_variance   | 0.176       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.291       |\n",
            "|    n_updates            | 1350        |\n",
            "|    policy_gradient_loss | -0.00153    |\n",
            "|    std                  | 0.405       |\n",
            "|    value_loss           | 0.654       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 117        |\n",
            "|    iterations           | 40         |\n",
            "|    time_elapsed         | 697        |\n",
            "|    total_timesteps      | 81920      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03722804 |\n",
            "|    clip_fraction        | 0.352      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.72      |\n",
            "|    explained_variance   | 0.55       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.361      |\n",
            "|    n_updates            | 1370       |\n",
            "|    policy_gradient_loss | 0.00955    |\n",
            "|    std                  | 0.403      |\n",
            "|    value_loss           | 0.665      |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=86896, episode_reward=-3448.41 +/- 3455.24\n",
            "Episode length: 8050.00 +/- 3902.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.05e+03    |\n",
            "|    mean_reward          | -3.45e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 86896       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018831737 |\n",
            "|    clip_fraction        | 0.282       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.53       |\n",
            "|    explained_variance   | 0.769       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.232       |\n",
            "|    n_updates            | 1400        |\n",
            "|    policy_gradient_loss | -0.00018    |\n",
            "|    std                  | 0.394       |\n",
            "|    value_loss           | 0.542       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=96896, episode_reward=-1271.11 +/- 1275.61\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.27e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 96896       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031725675 |\n",
            "|    clip_fraction        | 0.35        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.4        |\n",
            "|    explained_variance   | -0.324      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.214       |\n",
            "|    n_updates            | 1450        |\n",
            "|    policy_gradient_loss | 0.0128      |\n",
            "|    std                  | 0.39        |\n",
            "|    value_loss           | 0.541       |\n",
            "-----------------------------------------\n",
            "mean reward: -305.8447491999999, std reward: +/-1093.101941755418\n",
            "Training itteration  3\n",
            "Eval num_timesteps=6544, episode_reward=-1122.27 +/- 1388.94\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.12e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 6544        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030725328 |\n",
            "|    clip_fraction        | 0.327       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.21       |\n",
            "|    explained_variance   | 0.179       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.103       |\n",
            "|    n_updates            | 1500        |\n",
            "|    policy_gradient_loss | 0.00393     |\n",
            "|    std                  | 0.385       |\n",
            "|    value_loss           | 0.429       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=16544, episode_reward=-1122.16 +/- 1841.69\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.12e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16544       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021334395 |\n",
            "|    clip_fraction        | 0.322       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.01       |\n",
            "|    explained_variance   | -0.13       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.144       |\n",
            "|    n_updates            | 1550        |\n",
            "|    policy_gradient_loss | 0.00388     |\n",
            "|    std                  | 0.38        |\n",
            "|    value_loss           | 0.635       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 116        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 176        |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.21584626 |\n",
            "|    clip_fraction        | 0.348      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.98      |\n",
            "|    explained_variance   | 0.789      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.359      |\n",
            "|    n_updates            | 1560       |\n",
            "|    policy_gradient_loss | 0.00756    |\n",
            "|    std                  | 0.379      |\n",
            "|    value_loss           | 2.78       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=26544, episode_reward=-1017.31 +/- 774.47\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.02e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 26544       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026890304 |\n",
            "|    clip_fraction        | 0.291       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.7        |\n",
            "|    explained_variance   | 0.0484      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.108       |\n",
            "|    n_updates            | 1590        |\n",
            "|    policy_gradient_loss | 0.00623     |\n",
            "|    std                  | 0.372       |\n",
            "|    value_loss           | 0.175       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=36544, episode_reward=-1108.43 +/- 763.93\n",
            "Episode length: 8232.80 +/- 3536.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.23e+03    |\n",
            "|    mean_reward          | -1.11e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 36544       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024669023 |\n",
            "|    clip_fraction        | 0.272       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.28       |\n",
            "|    explained_variance   | 0.052       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.14        |\n",
            "|    n_updates            | 1640        |\n",
            "|    policy_gradient_loss | -0.00744    |\n",
            "|    std                  | 0.357       |\n",
            "|    value_loss           | 0.272       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 342        |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04255081 |\n",
            "|    clip_fraction        | 0.52       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.15      |\n",
            "|    explained_variance   | -0.00274   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.217      |\n",
            "|    n_updates            | 1660       |\n",
            "|    policy_gradient_loss | 0.0413     |\n",
            "|    std                  | 0.355      |\n",
            "|    value_loss           | 0.56       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=46544, episode_reward=-112.05 +/- 1354.19\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -112        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 46544       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027901445 |\n",
            "|    clip_fraction        | 0.346       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.12       |\n",
            "|    explained_variance   | 0.00192     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.145       |\n",
            "|    n_updates            | 1690        |\n",
            "|    policy_gradient_loss | 0.0198      |\n",
            "|    std                  | 0.352       |\n",
            "|    value_loss           | 0.339       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=56544, episode_reward=-1573.05 +/- 1076.77\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -1.57e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 56544     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0320884 |\n",
            "|    clip_fraction        | 0.408     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.97     |\n",
            "|    explained_variance   | 0.686     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.521     |\n",
            "|    n_updates            | 1740      |\n",
            "|    policy_gradient_loss | 0.0155    |\n",
            "|    std                  | 0.345     |\n",
            "|    value_loss           | 4.47      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 118        |\n",
            "|    iterations           | 30         |\n",
            "|    time_elapsed         | 520        |\n",
            "|    total_timesteps      | 61440      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02001179 |\n",
            "|    clip_fraction        | 0.351      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.86      |\n",
            "|    explained_variance   | 0.0339     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.091      |\n",
            "|    n_updates            | 1760       |\n",
            "|    policy_gradient_loss | 0.00454    |\n",
            "|    std                  | 0.339      |\n",
            "|    value_loss           | 0.24       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=66544, episode_reward=-1688.68 +/- 1300.05\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.69e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 66544       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017368712 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | 0.00731     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.199       |\n",
            "|    n_updates            | 1790        |\n",
            "|    policy_gradient_loss | 0.00723     |\n",
            "|    std                  | 0.328       |\n",
            "|    value_loss           | 0.267       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=76544, episode_reward=-1077.68 +/- 915.01\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | -1.08e+03  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 76544      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03773988 |\n",
            "|    clip_fraction        | 0.373      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.47      |\n",
            "|    explained_variance   | -0.00391   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.467      |\n",
            "|    n_updates            | 1840       |\n",
            "|    policy_gradient_loss | 0.0165     |\n",
            "|    std                  | 0.323      |\n",
            "|    value_loss           | 1.23       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.95e+04    |\n",
            "|    ep_rew_mean          | 3.59e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 117         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 699         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021899484 |\n",
            "|    clip_fraction        | 0.364       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.021       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.285       |\n",
            "|    n_updates            | 1860        |\n",
            "|    policy_gradient_loss | 0.0142      |\n",
            "|    std                  | 0.319       |\n",
            "|    value_loss           | 0.556       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=86544, episode_reward=-1356.53 +/- 1214.82\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.36e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 86544       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018995907 |\n",
            "|    clip_fraction        | 0.293       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0.0222      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.28        |\n",
            "|    n_updates            | 1890        |\n",
            "|    policy_gradient_loss | 0.00895     |\n",
            "|    std                  | 0.31        |\n",
            "|    value_loss           | 0.482       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=96544, episode_reward=105.50 +/- 412.41\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | 106        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 96544      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01767694 |\n",
            "|    clip_fraction        | 0.325      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.11      |\n",
            "|    explained_variance   | 0.00385    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.209      |\n",
            "|    n_updates            | 1940       |\n",
            "|    policy_gradient_loss | 0.0151     |\n",
            "|    std                  | 0.304      |\n",
            "|    value_loss           | 0.442      |\n",
            "----------------------------------------\n",
            "mean reward: -666.4786770000001, std reward: +/-972.0846196396051\n",
            "Training itteration  4\n",
            "Eval num_timesteps=6192, episode_reward=-549.88 +/- 1488.12\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -550        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 6192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028351387 |\n",
            "|    clip_fraction        | 0.331       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.796      |\n",
            "|    explained_variance   | 0.467       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.143       |\n",
            "|    n_updates            | 1990        |\n",
            "|    policy_gradient_loss | -0.009      |\n",
            "|    std                  | 0.296       |\n",
            "|    value_loss           | 0.319       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=16192, episode_reward=-26.77 +/- 316.21\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -26.8     |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 16192     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0239385 |\n",
            "|    clip_fraction        | 0.29      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.804    |\n",
            "|    explained_variance   | -0.00109  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.447     |\n",
            "|    n_updates            | 2030      |\n",
            "|    policy_gradient_loss | 0.0127    |\n",
            "|    std                  | 0.297     |\n",
            "|    value_loss           | 0.839     |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 115         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 177         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.046311583 |\n",
            "|    clip_fraction        | 0.452       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.798      |\n",
            "|    explained_variance   | 0.429       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.121       |\n",
            "|    n_updates            | 2050        |\n",
            "|    policy_gradient_loss | 0.0318      |\n",
            "|    std                  | 0.298       |\n",
            "|    value_loss           | 0.306       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=26192, episode_reward=-1083.19 +/- 1592.96\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.08e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 26192       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030405475 |\n",
            "|    clip_fraction        | 0.34        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.753      |\n",
            "|    explained_variance   | 0.535       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.286       |\n",
            "|    n_updates            | 2080        |\n",
            "|    policy_gradient_loss | 0.00207     |\n",
            "|    std                  | 0.297       |\n",
            "|    value_loss           | 0.57        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=36192, episode_reward=-556.13 +/- 1234.07\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -556        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 36192       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025438732 |\n",
            "|    clip_fraction        | 0.319       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.416      |\n",
            "|    explained_variance   | 0.0388      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.086       |\n",
            "|    n_updates            | 2130        |\n",
            "|    policy_gradient_loss | 0.00895     |\n",
            "|    std                  | 0.289       |\n",
            "|    value_loss           | 0.081       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 115        |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 355        |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03148728 |\n",
            "|    clip_fraction        | 0.488      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.333     |\n",
            "|    explained_variance   | 0.000675   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.164      |\n",
            "|    n_updates            | 2150       |\n",
            "|    policy_gradient_loss | 0.0454     |\n",
            "|    std                  | 0.287      |\n",
            "|    value_loss           | 0.342      |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=46192, episode_reward=-786.39 +/- 879.74\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -786        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 46192       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035007905 |\n",
            "|    clip_fraction        | 0.323       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.268      |\n",
            "|    explained_variance   | 0.000453    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.257       |\n",
            "|    n_updates            | 2180        |\n",
            "|    policy_gradient_loss | 0.0136      |\n",
            "|    std                  | 0.283       |\n",
            "|    value_loss           | 0.455       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=56192, episode_reward=-1566.57 +/- 1814.01\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.57e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56192       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.122831926 |\n",
            "|    clip_fraction        | 0.665       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.273      |\n",
            "|    explained_variance   | -0.051      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0922      |\n",
            "|    n_updates            | 2230        |\n",
            "|    policy_gradient_loss | 0.0429      |\n",
            "|    std                  | 0.284       |\n",
            "|    value_loss           | 0.0434      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 115         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 532         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027353425 |\n",
            "|    clip_fraction        | 0.378       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.277      |\n",
            "|    explained_variance   | 0.868       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0672      |\n",
            "|    n_updates            | 2250        |\n",
            "|    policy_gradient_loss | 0.00294     |\n",
            "|    std                  | 0.282       |\n",
            "|    value_loss           | 0.181       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=66192, episode_reward=-164.00 +/- 283.29\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -164      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 66192     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0473932 |\n",
            "|    clip_fraction        | 0.389     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.264    |\n",
            "|    explained_variance   | 0.957     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.347     |\n",
            "|    n_updates            | 2280      |\n",
            "|    policy_gradient_loss | 0.0215    |\n",
            "|    std                  | 0.284     |\n",
            "|    value_loss           | 1.13      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=76192, episode_reward=119.14 +/- 611.65\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | 119         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 76192       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026185863 |\n",
            "|    clip_fraction        | 0.365       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.282      |\n",
            "|    explained_variance   | 0.00677     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.486       |\n",
            "|    n_updates            | 2330        |\n",
            "|    policy_gradient_loss | 0.0261      |\n",
            "|    std                  | 0.286       |\n",
            "|    value_loss           | 0.991       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 115         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 709         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028503723 |\n",
            "|    clip_fraction        | 0.367       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.186      |\n",
            "|    explained_variance   | 0.207       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.173       |\n",
            "|    n_updates            | 2350        |\n",
            "|    policy_gradient_loss | 0.00289     |\n",
            "|    std                  | 0.282       |\n",
            "|    value_loss           | 0.42        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=86192, episode_reward=-666.08 +/- 1138.96\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -666        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 86192       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024389068 |\n",
            "|    clip_fraction        | 0.309       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.14        |\n",
            "|    explained_variance   | 0.244       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.12        |\n",
            "|    n_updates            | 2380        |\n",
            "|    policy_gradient_loss | -0.00444    |\n",
            "|    std                  | 0.277       |\n",
            "|    value_loss           | 0.347       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=96192, episode_reward=-472.52 +/- 1475.98\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | -473       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 96192      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04249101 |\n",
            "|    clip_fraction        | 0.36       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.231      |\n",
            "|    explained_variance   | 0.00432    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.433      |\n",
            "|    n_updates            | 2420       |\n",
            "|    policy_gradient_loss | 0.0252     |\n",
            "|    std                  | 0.272      |\n",
            "|    value_loss           | 0.686      |\n",
            "----------------------------------------\n",
            "mean reward: -435.4703794, std reward: +/-882.6420994654555\n",
            "Training itteration  5\n",
            "Eval num_timesteps=5840, episode_reward=-668.67 +/- 817.66\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -669        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 5840        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024998564 |\n",
            "|    clip_fraction        | 0.367       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.205       |\n",
            "|    explained_variance   | -6.58e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.564       |\n",
            "|    n_updates            | 2470        |\n",
            "|    policy_gradient_loss | 0.0235      |\n",
            "|    std                  | 0.274       |\n",
            "|    value_loss           | 0.966       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=15840, episode_reward=-1196.91 +/- 1467.26\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+04      |\n",
            "|    mean_reward          | -1.2e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 15840      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02107981 |\n",
            "|    clip_fraction        | 0.295      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.223      |\n",
            "|    explained_variance   | 0.00235    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.179      |\n",
            "|    n_updates            | 2520       |\n",
            "|    policy_gradient_loss | 0.0162     |\n",
            "|    std                  | 0.27       |\n",
            "|    value_loss           | 0.326      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 116         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 176         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026424311 |\n",
            "|    clip_fraction        | 0.422       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.263       |\n",
            "|    explained_variance   | -0.00471    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.254       |\n",
            "|    n_updates            | 2540        |\n",
            "|    policy_gradient_loss | 0.0293      |\n",
            "|    std                  | 0.269       |\n",
            "|    value_loss           | 0.662       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=25840, episode_reward=-413.13 +/- 1202.02\n",
            "Episode length: 8001.00 +/- 4000.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 8e+03      |\n",
            "|    mean_reward          | -413       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 25840      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03295526 |\n",
            "|    clip_fraction        | 0.337      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.299      |\n",
            "|    explained_variance   | 0.00548    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.342      |\n",
            "|    n_updates            | 2570       |\n",
            "|    policy_gradient_loss | 0.0164     |\n",
            "|    std                  | 0.267      |\n",
            "|    value_loss           | 0.663      |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=35840, episode_reward=-874.00 +/- 1727.81\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| eval/                   |          |\n",
            "|    mean_ep_length       | 1e+04    |\n",
            "|    mean_reward          | -874     |\n",
            "| time/                   |          |\n",
            "|    total_timesteps      | 35840    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 27.56696 |\n",
            "|    clip_fraction        | 0.983    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | 0.327    |\n",
            "|    explained_variance   | 0.254    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.388    |\n",
            "|    n_updates            | 2620     |\n",
            "|    policy_gradient_loss | 0.309    |\n",
            "|    std                  | 0.266    |\n",
            "|    value_loss           | 0.249    |\n",
            "--------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 342         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025727186 |\n",
            "|    clip_fraction        | 0.477       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.273       |\n",
            "|    explained_variance   | 0.0291      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.425       |\n",
            "|    n_updates            | 2640        |\n",
            "|    policy_gradient_loss | 0.0421      |\n",
            "|    std                  | 0.267       |\n",
            "|    value_loss           | 0.74        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=45840, episode_reward=-1625.30 +/- 1206.52\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.63e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 45840       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039201517 |\n",
            "|    clip_fraction        | 0.305       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.277       |\n",
            "|    explained_variance   | 0.000844    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.345       |\n",
            "|    n_updates            | 2670        |\n",
            "|    policy_gradient_loss | 0.0167      |\n",
            "|    std                  | 0.266       |\n",
            "|    value_loss           | 0.407       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=55840, episode_reward=-1533.00 +/- 1537.71\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.53e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 55840       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034340765 |\n",
            "|    clip_fraction        | 0.419       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.418       |\n",
            "|    explained_variance   | 0.00871     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0234      |\n",
            "|    n_updates            | 2720        |\n",
            "|    policy_gradient_loss | 0.0158      |\n",
            "|    std                  | 0.264       |\n",
            "|    value_loss           | 0.0999      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 117        |\n",
            "|    iterations           | 30         |\n",
            "|    time_elapsed         | 522        |\n",
            "|    total_timesteps      | 61440      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03405881 |\n",
            "|    clip_fraction        | 0.431      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.488      |\n",
            "|    explained_variance   | 0.00119    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.145      |\n",
            "|    n_updates            | 2740       |\n",
            "|    policy_gradient_loss | 0.026      |\n",
            "|    std                  | 0.26       |\n",
            "|    value_loss           | 0.237      |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=65840, episode_reward=-1177.84 +/- 1688.58\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.18e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 65840       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030019145 |\n",
            "|    clip_fraction        | 0.312       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.481       |\n",
            "|    explained_variance   | 0.00576     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0862      |\n",
            "|    n_updates            | 2770        |\n",
            "|    policy_gradient_loss | 0.00676     |\n",
            "|    std                  | 0.26        |\n",
            "|    value_loss           | 0.171       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=75840, episode_reward=-812.55 +/- 1330.09\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -813        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 75840       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015525131 |\n",
            "|    clip_fraction        | 0.297       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.67        |\n",
            "|    explained_variance   | 0.106       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.182       |\n",
            "|    n_updates            | 2820        |\n",
            "|    policy_gradient_loss | 0.00473     |\n",
            "|    std                  | 0.257       |\n",
            "|    value_loss           | 0.315       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 116         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 702         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036897365 |\n",
            "|    clip_fraction        | 0.39        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.666       |\n",
            "|    explained_variance   | 0.0131      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0602      |\n",
            "|    n_updates            | 2840        |\n",
            "|    policy_gradient_loss | 0.0207      |\n",
            "|    std                  | 0.255       |\n",
            "|    value_loss           | 0.173       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=85840, episode_reward=-255.76 +/- 1094.58\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -256        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 85840       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.071671695 |\n",
            "|    clip_fraction        | 0.373       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.723       |\n",
            "|    explained_variance   | 0.0329      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0852      |\n",
            "|    n_updates            | 2860        |\n",
            "|    policy_gradient_loss | 0.0196      |\n",
            "|    std                  | 0.253       |\n",
            "|    value_loss           | 0.111       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=95840, episode_reward=119.53 +/- 950.87\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | 120       |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 95840     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1015.2988 |\n",
            "|    clip_fraction        | 0.996     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.72      |\n",
            "|    explained_variance   | 0.463     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.506     |\n",
            "|    n_updates            | 2910      |\n",
            "|    policy_gradient_loss | 0.319     |\n",
            "|    std                  | 0.253     |\n",
            "|    value_loss           | 0.587     |\n",
            "---------------------------------------\n",
            "mean reward: -1184.304781, std reward: +/-987.4322959878839\n",
            "Training itteration  6\n",
            "Eval num_timesteps=5488, episode_reward=-3455.32 +/- 3566.05\n",
            "Episode length: 8013.80 +/- 3974.40\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8.01e+03  |\n",
            "|    mean_reward          | -3.46e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 5488      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 154.12164 |\n",
            "|    clip_fraction        | 0.974     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.722     |\n",
            "|    explained_variance   | 0.349     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.07      |\n",
            "|    n_updates            | 2960      |\n",
            "|    policy_gradient_loss | 0.314     |\n",
            "|    std                  | 0.251     |\n",
            "|    value_loss           | 2.46      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=15488, episode_reward=-1149.85 +/- 1425.04\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -1.15e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 15488     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 721.2366  |\n",
            "|    clip_fraction        | 0.997     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.721     |\n",
            "|    explained_variance   | 0.381     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.84      |\n",
            "|    n_updates            | 3010      |\n",
            "|    policy_gradient_loss | 0.328     |\n",
            "|    std                  | 0.251     |\n",
            "|    value_loss           | 0.897     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 164        |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04019754 |\n",
            "|    clip_fraction        | 0.521      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.807      |\n",
            "|    explained_variance   | -0.00142   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.141      |\n",
            "|    n_updates            | 3030       |\n",
            "|    policy_gradient_loss | 0.0439     |\n",
            "|    std                  | 0.247      |\n",
            "|    value_loss           | 0.248      |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=25488, episode_reward=143.83 +/- 1470.10\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | 144         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 25488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.042591553 |\n",
            "|    clip_fraction        | 0.403       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.818       |\n",
            "|    explained_variance   | 0.008       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.147       |\n",
            "|    n_updates            | 3060        |\n",
            "|    policy_gradient_loss | 0.0122      |\n",
            "|    std                  | 0.247       |\n",
            "|    value_loss           | 0.228       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=35488, episode_reward=-865.48 +/- 1773.67\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -865      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 35488     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 288.25507 |\n",
            "|    clip_fraction        | 0.996     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.836     |\n",
            "|    explained_variance   | 0.064     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.517     |\n",
            "|    n_updates            | 3110      |\n",
            "|    policy_gradient_loss | 0.298     |\n",
            "|    std                  | 0.246     |\n",
            "|    value_loss           | 0.706     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 120      |\n",
            "|    iterations           | 20       |\n",
            "|    time_elapsed         | 340      |\n",
            "|    total_timesteps      | 40960    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 92.95505 |\n",
            "|    clip_fraction        | 0.99     |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | 0.828    |\n",
            "|    explained_variance   | 0.00977  |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.48     |\n",
            "|    n_updates            | 3130     |\n",
            "|    policy_gradient_loss | 0.319    |\n",
            "|    std                  | 0.246    |\n",
            "|    value_loss           | 0.338    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=45488, episode_reward=86.89 +/- 1263.00\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | 86.9        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 45488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031656273 |\n",
            "|    clip_fraction        | 0.398       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.949       |\n",
            "|    explained_variance   | -0.00277    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.176       |\n",
            "|    n_updates            | 3160        |\n",
            "|    policy_gradient_loss | 0.0265      |\n",
            "|    std                  | 0.241       |\n",
            "|    value_loss           | 0.321       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=55488, episode_reward=-1210.35 +/- 1246.38\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.21e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 55488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.037459202 |\n",
            "|    clip_fraction        | 0.289       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 1.06        |\n",
            "|    explained_variance   | 0.00658     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0602      |\n",
            "|    n_updates            | 3210        |\n",
            "|    policy_gradient_loss | 0.00616     |\n",
            "|    std                  | 0.236       |\n",
            "|    value_loss           | 0.186       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.12e+04    |\n",
            "|    ep_rew_mean          | 4.4e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 118         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 517         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032267973 |\n",
            "|    clip_fraction        | 0.451       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 1.1         |\n",
            "|    explained_variance   | 0.924       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.209       |\n",
            "|    n_updates            | 3230        |\n",
            "|    policy_gradient_loss | 0.0219      |\n",
            "|    std                  | 0.233       |\n",
            "|    value_loss           | 0.412       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=65488, episode_reward=-2808.26 +/- 3797.45\n",
            "Episode length: 8006.00 +/- 3990.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.01e+03    |\n",
            "|    mean_reward          | -2.81e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 65488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028130818 |\n",
            "|    clip_fraction        | 0.426       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 1.18        |\n",
            "|    explained_variance   | 0.741       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.326       |\n",
            "|    n_updates            | 3250        |\n",
            "|    policy_gradient_loss | 0.0147      |\n",
            "|    std                  | 0.231       |\n",
            "|    value_loss           | 0.633       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=75488, episode_reward=-1951.82 +/- 1476.60\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+04       |\n",
            "|    mean_reward          | -1.95e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 75488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029193608 |\n",
            "|    clip_fraction        | 0.36        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 1.12        |\n",
            "|    explained_variance   | 0.336       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0662      |\n",
            "|    n_updates            | 3300        |\n",
            "|    policy_gradient_loss | -0.00696    |\n",
            "|    std                  | 0.23        |\n",
            "|    value_loss           | 0.228       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 5.12e+04  |\n",
            "|    ep_rew_mean          | 4.4e+03   |\n",
            "| time/                   |           |\n",
            "|    fps                  | 120       |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 680       |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1082.6107 |\n",
            "|    clip_fraction        | 0.994     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.06      |\n",
            "|    explained_variance   | 0.0656    |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.668     |\n",
            "|    n_updates            | 3330      |\n",
            "|    policy_gradient_loss | 0.303     |\n",
            "|    std                  | 0.232     |\n",
            "|    value_loss           | 1.15      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=85488, episode_reward=-3178.45 +/- 3538.60\n",
            "Episode length: 8013.80 +/- 3974.40\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8.01e+03  |\n",
            "|    mean_reward          | -3.18e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 85488     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 783.7138  |\n",
            "|    clip_fraction        | 0.992     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.06      |\n",
            "|    explained_variance   | 0.933     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.764     |\n",
            "|    n_updates            | 3350      |\n",
            "|    policy_gradient_loss | 0.319     |\n",
            "|    std                  | 0.232     |\n",
            "|    value_loss           | 1.71      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=95488, episode_reward=-1702.50 +/- 4148.27\n",
            "Episode length: 8013.80 +/- 3974.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.01e+03    |\n",
            "|    mean_reward          | -1.7e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 95488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018191222 |\n",
            "|    clip_fraction        | 0.349       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 1.03        |\n",
            "|    explained_variance   | 0.000157    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.176       |\n",
            "|    n_updates            | 3400        |\n",
            "|    policy_gradient_loss | 0.0241      |\n",
            "|    std                  | 0.232       |\n",
            "|    value_loss           | 0.241       |\n",
            "-----------------------------------------\n",
            "mean reward: -4313.874332199999, std reward: +/-4710.344065010757\n",
            "Training itteration  7\n",
            "Eval num_timesteps=5136, episode_reward=-606.25 +/- 1515.04\n",
            "Episode length: 8001.00 +/- 4000.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8e+03     |\n",
            "|    mean_reward          | -606      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 5136      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 4874.2217 |\n",
            "|    clip_fraction        | 0.996     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.05      |\n",
            "|    explained_variance   | 0.0187    |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.789     |\n",
            "|    n_updates            | 3450      |\n",
            "|    policy_gradient_loss | 0.321     |\n",
            "|    std                  | 0.231     |\n",
            "|    value_loss           | 1.41      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=15136, episode_reward=-4623.61 +/- 4466.27\n",
            "Episode length: 4026.60 +/- 4878.13\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 4.03e+03  |\n",
            "|    mean_reward          | -4.62e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 15136     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1382.0203 |\n",
            "|    clip_fraction        | 0.988     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.05      |\n",
            "|    explained_variance   | 0.461     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.33      |\n",
            "|    n_updates            | 3500      |\n",
            "|    policy_gradient_loss | 0.314     |\n",
            "|    std                  | 0.231     |\n",
            "|    value_loss           | 2.38      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 3.83e+03  |\n",
            "|    ep_rew_mean          | -1.07e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 163       |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 125       |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 17.867998 |\n",
            "|    clip_fraction        | 0.613     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.05      |\n",
            "|    explained_variance   | -0.385    |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 14.8      |\n",
            "|    n_updates            | 3520      |\n",
            "|    policy_gradient_loss | 0.0738    |\n",
            "|    std                  | 0.231     |\n",
            "|    value_loss           | 212       |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=25136, episode_reward=-8227.88 +/- 3560.20\n",
            "Episode length: 2052.20 +/- 3974.40\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 2.05e+03  |\n",
            "|    mean_reward          | -8.23e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 25136     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1.9660478 |\n",
            "|    clip_fraction        | 0.722     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.07      |\n",
            "|    explained_variance   | 0.0176    |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.28      |\n",
            "|    n_updates            | 3550      |\n",
            "|    policy_gradient_loss | 0.253     |\n",
            "|    std                  | 0.231     |\n",
            "|    value_loss           | 116       |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=35136, episode_reward=-1581.50 +/- 4342.30\n",
            "Episode length: 6013.80 +/- 4883.34\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 6.01e+03    |\n",
            "|    mean_reward          | -1.58e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 35136       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019893125 |\n",
            "|    clip_fraction        | 0.315       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 1.23        |\n",
            "|    explained_variance   | -0.0551     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.127       |\n",
            "|    n_updates            | 3600        |\n",
            "|    policy_gradient_loss | 0.0122      |\n",
            "|    std                  | 0.223       |\n",
            "|    value_loss           | 0.266       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 3.83e+03  |\n",
            "|    ep_rew_mean          | -1.07e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 181       |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 225       |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 24475.02  |\n",
            "|    clip_fraction        | 0.963     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.27      |\n",
            "|    explained_variance   | 0.000635  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 385       |\n",
            "|    n_updates            | 3620      |\n",
            "|    policy_gradient_loss | 0.18      |\n",
            "|    std                  | 0.223     |\n",
            "|    value_loss           | 2.17e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=45136, episode_reward=-2565.40 +/- 3979.12\n",
            "Episode length: 8013.80 +/- 3974.40\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8.01e+03  |\n",
            "|    mean_reward          | -2.57e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 45136     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 87.42787  |\n",
            "|    clip_fraction        | 0.97      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.27      |\n",
            "|    explained_variance   | 0.00049   |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.82      |\n",
            "|    n_updates            | 3650      |\n",
            "|    policy_gradient_loss | 0.172     |\n",
            "|    std                  | 0.223     |\n",
            "|    value_loss           | 751       |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=55136, episode_reward=-6765.51 +/- 3988.26\n",
            "Episode length: 4039.40 +/- 4867.63\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 4.04e+03  |\n",
            "|    mean_reward          | -6.77e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 55136     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2397.9307 |\n",
            "|    clip_fraction        | 0.985     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.27      |\n",
            "|    explained_variance   | -0.00466  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.38      |\n",
            "|    n_updates            | 3690      |\n",
            "|    policy_gradient_loss | 0.234     |\n",
            "|    std                  | 0.223     |\n",
            "|    value_loss           | 42.2      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 6.92e+03  |\n",
            "|    ep_rew_mean          | -1.11e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 174       |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 351       |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2920.522  |\n",
            "|    clip_fraction        | 0.928     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.27      |\n",
            "|    explained_variance   | -0.00554  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.19e+03  |\n",
            "|    n_updates            | 3720      |\n",
            "|    policy_gradient_loss | 0.116     |\n",
            "|    std                  | 0.223     |\n",
            "|    value_loss           | 4.72e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=65136, episode_reward=-6111.92 +/- 4788.68\n",
            "Episode length: 4039.40 +/- 4867.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 4.04e+03    |\n",
            "|    mean_reward          | -6.11e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 65136       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036527894 |\n",
            "|    clip_fraction        | 0.245       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 1.27        |\n",
            "|    explained_variance   | 0.354       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.44e+04    |\n",
            "|    n_updates            | 3740        |\n",
            "|    policy_gradient_loss | 0.00459     |\n",
            "|    std                  | 0.223       |\n",
            "|    value_loss           | 1.29e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=75136, episode_reward=-4208.97 +/- 4820.10\n",
            "Episode length: 6026.60 +/- 4867.63\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 6.03e+03  |\n",
            "|    mean_reward          | -4.21e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 75136     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 4.286288  |\n",
            "|    clip_fraction        | 0.707     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.24      |\n",
            "|    explained_variance   | 0.434     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 34.4      |\n",
            "|    n_updates            | 3790      |\n",
            "|    policy_gradient_loss | 0.126     |\n",
            "|    std                  | 0.224     |\n",
            "|    value_loss           | 102       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 6.84e+03  |\n",
            "|    ep_rew_mean          | -1.11e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 175       |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 465       |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 10569.321 |\n",
            "|    clip_fraction        | 0.969     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.21      |\n",
            "|    explained_variance   | 0.00949   |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 271       |\n",
            "|    n_updates            | 3820      |\n",
            "|    policy_gradient_loss | 0.15      |\n",
            "|    std                  | 0.224     |\n",
            "|    value_loss           | 2.54e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=85136, episode_reward=-4119.21 +/- 4866.56\n",
            "Episode length: 4039.60 +/- 4867.46\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 4.04e+03  |\n",
            "|    mean_reward          | -4.12e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 85136     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 189.15746 |\n",
            "|    clip_fraction        | 0.971     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.21      |\n",
            "|    explained_variance   | -0.0183   |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 21.1      |\n",
            "|    n_updates            | 3840      |\n",
            "|    policy_gradient_loss | 0.156     |\n",
            "|    std                  | 0.224     |\n",
            "|    value_loss           | 1.46e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=95136, episode_reward=-3851.81 +/- 5035.73\n",
            "Episode length: 4026.60 +/- 4878.13\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 4.03e+03  |\n",
            "|    mean_reward          | -3.85e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 95136     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 19.788895 |\n",
            "|    clip_fraction        | 0.716     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.15      |\n",
            "|    explained_variance   | 0.339     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 63        |\n",
            "|    n_updates            | 3890      |\n",
            "|    policy_gradient_loss | 0.154     |\n",
            "|    std                  | 0.226     |\n",
            "|    value_loss           | 607       |\n",
            "---------------------------------------\n",
            "mean reward: -2775.7436331999997, std reward: +/-3710.553290221801\n",
            "Training itteration  8\n",
            "Eval num_timesteps=4784, episode_reward=-3881.58 +/- 5008.95\n",
            "Episode length: 6027.00 +/- 4867.14\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 6.03e+03  |\n",
            "|    mean_reward          | -3.88e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 4784      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 35.8924   |\n",
            "|    clip_fraction        | 0.965     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.1       |\n",
            "|    explained_variance   | 0.177     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 3.81      |\n",
            "|    n_updates            | 3940      |\n",
            "|    policy_gradient_loss | 0.173     |\n",
            "|    std                  | 0.227     |\n",
            "|    value_loss           | 1.45e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=14784, episode_reward=-59.86 +/- 1335.56\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -59.9     |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 14784     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.8230753 |\n",
            "|    clip_fraction        | 0.552     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 1.04      |\n",
            "|    explained_variance   | 0.877     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 81.8      |\n",
            "|    n_updates            | 3990      |\n",
            "|    policy_gradient_loss | 0.0778    |\n",
            "|    std                  | 0.228     |\n",
            "|    value_loss           | 247       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.24e+03  |\n",
            "|    ep_rew_mean          | -1.01e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 136       |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 149       |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 100.34277 |\n",
            "|    clip_fraction        | 0.984     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.94      |\n",
            "|    explained_variance   | -0.291    |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.389     |\n",
            "|    n_updates            | 4010      |\n",
            "|    policy_gradient_loss | 0.285     |\n",
            "|    std                  | 0.23      |\n",
            "|    value_loss           | 0.888     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=24784, episode_reward=-8129.74 +/- 3777.97\n",
            "Episode length: 2152.40 +/- 3926.03\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 2.15e+03  |\n",
            "|    mean_reward          | -8.13e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 24784     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 103.49315 |\n",
            "|    clip_fraction        | 0.979     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.866     |\n",
            "|    explained_variance   | 0.613     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.934     |\n",
            "|    n_updates            | 4040      |\n",
            "|    policy_gradient_loss | 0.307     |\n",
            "|    std                  | 0.232     |\n",
            "|    value_loss           | 2.14      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=34784, episode_reward=-5614.75 +/- 5405.30\n",
            "Episode length: 4037.20 +/- 4869.42\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 4.04e+03   |\n",
            "|    mean_reward          | -5.61e+03  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 34784      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.10240528 |\n",
            "|    clip_fraction        | 0.274      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.835      |\n",
            "|    explained_variance   | 0.823      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 130        |\n",
            "|    n_updates            | 4080       |\n",
            "|    policy_gradient_loss | 0.0167     |\n",
            "|    std                  | 0.233      |\n",
            "|    value_loss           | 1.42e+03   |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 3.59e+03  |\n",
            "|    ep_rew_mean          | -8.87e+03 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 172       |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 237       |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 15.601792 |\n",
            "|    clip_fraction        | 0.781     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.832     |\n",
            "|    explained_variance   | 0.688     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 11.5      |\n",
            "|    n_updates            | 4110      |\n",
            "|    policy_gradient_loss | 0.181     |\n",
            "|    std                  | 0.233     |\n",
            "|    value_loss           | 372       |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=44784, episode_reward=-8010.47 +/- 4003.52\n",
            "Episode length: 72.40 +/- 24.90\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 72.4      |\n",
            "|    mean_reward          | -8.01e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 44784     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 45.713276 |\n",
            "|    clip_fraction        | 0.826     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.813     |\n",
            "|    explained_variance   | -0.0632   |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 16.3      |\n",
            "|    n_updates            | 4130      |\n",
            "|    policy_gradient_loss | 0.153     |\n",
            "|    std                  | 0.233     |\n",
            "|    value_loss           | 129       |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=54784, episode_reward=-1909.32 +/- 4129.74\n",
            "Episode length: 8013.80 +/- 3974.40\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8.01e+03  |\n",
            "|    mean_reward          | -1.91e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 54784     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1153.2986 |\n",
            "|    clip_fraction        | 0.885     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.812     |\n",
            "|    explained_variance   | -3.37     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 6.55e+03  |\n",
            "|    n_updates            | 4180      |\n",
            "|    policy_gradient_loss | 0.111     |\n",
            "|    std                  | 0.233     |\n",
            "|    value_loss           | 1.71e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 6.4e+03   |\n",
            "|    ep_rew_mean          | -9.45e+03 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 182       |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 337       |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 41.151924 |\n",
            "|    clip_fraction        | 0.94      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.811     |\n",
            "|    explained_variance   | 0.0486    |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.7       |\n",
            "|    n_updates            | 4210      |\n",
            "|    policy_gradient_loss | 0.142     |\n",
            "|    std                  | 0.233     |\n",
            "|    value_loss           | 6.28e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=64784, episode_reward=990.37 +/- 373.17\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | 990       |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 64784     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 420.08484 |\n",
            "|    clip_fraction        | 0.955     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.811     |\n",
            "|    explained_variance   | 0.845     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 102       |\n",
            "|    n_updates            | 4230      |\n",
            "|    policy_gradient_loss | 0.274     |\n",
            "|    std                  | 0.233     |\n",
            "|    value_loss           | 1.64e+03  |\n",
            "---------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=74784, episode_reward=-1555.65 +/- 1665.39\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -1.56e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 74784     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 53.761147 |\n",
            "|    clip_fraction        | 0.969     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.808     |\n",
            "|    explained_variance   | 0.312     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 87.2      |\n",
            "|    n_updates            | 4280      |\n",
            "|    policy_gradient_loss | 0.163     |\n",
            "|    std                  | 0.233     |\n",
            "|    value_loss           | 2.26e+03  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 6.4e+03   |\n",
            "|    ep_rew_mean          | -9.45e+03 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 159       |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 512       |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 251.39236 |\n",
            "|    clip_fraction        | 0.994     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.805     |\n",
            "|    explained_variance   | 0.739     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.46      |\n",
            "|    n_updates            | 4310      |\n",
            "|    policy_gradient_loss | 0.309     |\n",
            "|    std                  | 0.234     |\n",
            "|    value_loss           | 13.7      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=84784, episode_reward=-714.44 +/- 1142.33\n",
            "Episode length: 8001.00 +/- 4000.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8e+03     |\n",
            "|    mean_reward          | -714      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 84784     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1772.9851 |\n",
            "|    clip_fraction        | 0.986     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.788     |\n",
            "|    explained_variance   | 0.213     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.04      |\n",
            "|    n_updates            | 4330      |\n",
            "|    policy_gradient_loss | 0.27      |\n",
            "|    std                  | 0.234     |\n",
            "|    value_loss           | 12.5      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=94784, episode_reward=-1599.46 +/- 2225.97\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| eval/                   |          |\n",
            "|    mean_ep_length       | 1e+04    |\n",
            "|    mean_reward          | -1.6e+03 |\n",
            "| time/                   |          |\n",
            "|    total_timesteps      | 94784    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 45.96821 |\n",
            "|    clip_fraction        | 0.986    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | 0.772    |\n",
            "|    explained_variance   | 0.431    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 4.64     |\n",
            "|    n_updates            | 4380     |\n",
            "|    policy_gradient_loss | 0.307    |\n",
            "|    std                  | 0.235    |\n",
            "|    value_loss           | 24.9     |\n",
            "--------------------------------------\n",
            "mean reward: -2339.9114116, std reward: +/-4164.786296790056\n",
            "Training itteration  9\n",
            "Eval num_timesteps=4432, episode_reward=-2996.91 +/- 3633.67\n",
            "Episode length: 8060.80 +/- 3880.40\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8.06e+03  |\n",
            "|    mean_reward          | -3e+03    |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 4432      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 677.35425 |\n",
            "|    clip_fraction        | 0.995     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.727     |\n",
            "|    explained_variance   | 0.528     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.66      |\n",
            "|    n_updates            | 4430      |\n",
            "|    policy_gradient_loss | 0.285     |\n",
            "|    std                  | 0.236     |\n",
            "|    value_loss           | 2.54      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=14432, episode_reward=-5849.57 +/- 3410.79\n",
            "Episode length: 6120.60 +/- 4752.50\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 6.12e+03  |\n",
            "|    mean_reward          | -5.85e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 14432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 7835.414  |\n",
            "|    clip_fraction        | 0.972     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.723     |\n",
            "|    explained_variance   | 0.71      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 7.41      |\n",
            "|    n_updates            | 4480      |\n",
            "|    policy_gradient_loss | 0.239     |\n",
            "|    std                  | 0.236     |\n",
            "|    value_loss           | 532       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 7.36e+03  |\n",
            "|    ep_rew_mean          | -1.12e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 147       |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 138       |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 629.55237 |\n",
            "|    clip_fraction        | 0.991     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.723     |\n",
            "|    explained_variance   | 0.709     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 6.15      |\n",
            "|    n_updates            | 4500      |\n",
            "|    policy_gradient_loss | 0.29      |\n",
            "|    std                  | 0.236     |\n",
            "|    value_loss           | 73.8      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=24432, episode_reward=-2379.25 +/- 1987.48\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -2.38e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 24432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 3882.9385 |\n",
            "|    clip_fraction        | 0.989     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.72      |\n",
            "|    explained_variance   | 0.944     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 96        |\n",
            "|    n_updates            | 4520      |\n",
            "|    policy_gradient_loss | 0.287     |\n",
            "|    std                  | 0.236     |\n",
            "|    value_loss           | 96.5      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=34432, episode_reward=-4578.52 +/- 4553.10\n",
            "Episode length: 6129.00 +/- 4742.23\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 6.13e+03  |\n",
            "|    mean_reward          | -4.58e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 34432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 176.01791 |\n",
            "|    clip_fraction        | 0.978     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.707     |\n",
            "|    explained_variance   | -0.000906 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 122       |\n",
            "|    n_updates            | 4570      |\n",
            "|    policy_gradient_loss | 0.132     |\n",
            "|    std                  | 0.236     |\n",
            "|    value_loss           | 519       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 8.26e+03  |\n",
            "|    ep_rew_mean          | -1.08e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 141       |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 289       |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 228.23758 |\n",
            "|    clip_fraction        | 0.986     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.699     |\n",
            "|    explained_variance   | 0.776     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 719       |\n",
            "|    n_updates            | 4600      |\n",
            "|    policy_gradient_loss | 0.229     |\n",
            "|    std                  | 0.237     |\n",
            "|    value_loss           | 1.77e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=44432, episode_reward=-5254.44 +/- 3916.37\n",
            "Episode length: 6138.40 +/- 4730.86\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 6.14e+03  |\n",
            "|    mean_reward          | -5.25e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 44432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1111.58   |\n",
            "|    clip_fraction        | 0.983     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.697     |\n",
            "|    explained_variance   | 0.764     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 15.1      |\n",
            "|    n_updates            | 4620      |\n",
            "|    policy_gradient_loss | 0.283     |\n",
            "|    std                  | 0.237     |\n",
            "|    value_loss           | 76.1      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=54432, episode_reward=-935.78 +/- 918.05\n",
            "Episode length: 8002.60 +/- 3996.80\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8e+03     |\n",
            "|    mean_reward          | -936      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 54432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 142.75519 |\n",
            "|    clip_fraction        | 0.966     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.634     |\n",
            "|    explained_variance   | 0.727     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 7.07      |\n",
            "|    n_updates            | 4670      |\n",
            "|    policy_gradient_loss | 0.288     |\n",
            "|    std                  | 0.239     |\n",
            "|    value_loss           | 17.7      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.09e+04  |\n",
            "|    ep_rew_mean          | -9.6e+03  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 143       |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 427       |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 324.37704 |\n",
            "|    clip_fraction        | 0.966     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.612     |\n",
            "|    explained_variance   | -0.13     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 101       |\n",
            "|    n_updates            | 4700      |\n",
            "|    policy_gradient_loss | 0.19      |\n",
            "|    std                  | 0.24      |\n",
            "|    value_loss           | 1.73e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=64432, episode_reward=-1823.42 +/- 1146.73\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -1.82e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 64432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 360.78674 |\n",
            "|    clip_fraction        | 0.978     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.612     |\n",
            "|    explained_variance   | -0.164    |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 23.2      |\n",
            "|    n_updates            | 4720      |\n",
            "|    policy_gradient_loss | 0.169     |\n",
            "|    std                  | 0.24      |\n",
            "|    value_loss           | 694       |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=74432, episode_reward=-2721.90 +/- 3979.93\n",
            "Episode length: 8015.00 +/- 3972.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8.02e+03  |\n",
            "|    mean_reward          | -2.72e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 74432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 918.3516  |\n",
            "|    clip_fraction        | 0.993     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.605     |\n",
            "|    explained_variance   | 0.702     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.747     |\n",
            "|    n_updates            | 4770      |\n",
            "|    policy_gradient_loss | 0.304     |\n",
            "|    std                  | 0.24      |\n",
            "|    value_loss           | 1.36      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 9.4e+03   |\n",
            "|    ep_rew_mean          | -9.87e+03 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 138       |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 591       |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 4068.7705 |\n",
            "|    clip_fraction        | 0.964     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.61      |\n",
            "|    explained_variance   | 0.44      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 398       |\n",
            "|    n_updates            | 4800      |\n",
            "|    policy_gradient_loss | 0.294     |\n",
            "|    std                  | 0.24      |\n",
            "|    value_loss           | 2.44e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=84432, episode_reward=-1684.90 +/- 1795.37\n",
            "Episode length: 10001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+04     |\n",
            "|    mean_reward          | -1.68e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 84432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 103.77734 |\n",
            "|    clip_fraction        | 0.98      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.61      |\n",
            "|    explained_variance   | 0.799     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 82        |\n",
            "|    n_updates            | 4820      |\n",
            "|    policy_gradient_loss | 0.258     |\n",
            "|    std                  | 0.24      |\n",
            "|    value_loss           | 1.22e+03  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=94432, episode_reward=-1464.93 +/- 1295.57\n",
            "Episode length: 8001.00 +/- 4000.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 8e+03     |\n",
            "|    mean_reward          | -1.46e+03 |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 94432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2169.316  |\n",
            "|    clip_fraction        | 0.992     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.622     |\n",
            "|    explained_variance   | 0.738     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 5.9       |\n",
            "|    n_updates            | 4870      |\n",
            "|    policy_gradient_loss | 0.241     |\n",
            "|    std                  | 0.239     |\n",
            "|    value_loss           | 26        |\n",
            "---------------------------------------\n",
            "mean reward: -1019.3017427999999, std reward: +/-625.3792137814426\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import os\n",
        "import pandas as pd\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "\n",
        "\n",
        "path_save = \"/content/\"\n",
        "model = PPO('MultiInputPolicy', env,learning_rate=3e-4,policy_kwargs=policy_kwargs, verbose=1, batch_size=64)\n",
        "print(\"Model created\")\n",
        "iteraciones = []\n",
        "recompensa_promedio=[]\n",
        "\n",
        "os.makedirs(path_save, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mean_rewards = []\n",
        "for i in range(10):\n",
        "  print(\"Training itteration \",i)\n",
        "  model.learn(total_timesteps=100000,log_interval = 10,callback=eval_callback)\n",
        "  # Save the agent\n",
        "\n",
        "  model.save(path_save+\"PPO_Ant\")\n",
        "  mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)\n",
        "  mean_rewards.append(mean_reward)\n",
        "  print(f'mean reward: {mean_reward}, std reward: +/-{std_reward}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "UhUc15U45rYE",
        "outputId": "9a1b8a03-c375-4df3-bb2a-648ba351b544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Observation spaces do not match: Box([-10.        -10.        -10.         -3.1415927  -3.1415927  -3.1415927\n -10.        -10.        -20.        -20.        -20.        -20.\n -20.        -20.       ], [10.        10.        10.         3.1415927  3.1415927  3.1415927\n 10.        10.        20.        20.        20.        20.\n 20.        20.       ], (14,), float32) != Dict('desired_goal': Box(-inf, inf, (2,), float32), 'observation': Box(-inf, inf, (20,), float32))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-28b23f4a7d08>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Ant_custom/SAC_Ant_ayudame_dios.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Ant_custom/SAC_Ant_ayudame_dios.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"verbose\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;31m# Check if given env is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m             \u001b[0mcheck_for_correct_spaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observation_space\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action_space\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m             \u001b[0;31m# Discard `_last_obs`, this will force the env to reset before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;31m# See issue https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/utils.py\u001b[0m in \u001b[0;36mcheck_for_correct_spaces\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobservation_space\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Observation spaces do not match: {observation_space} != {env.observation_space}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maction_space\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action spaces do not match: {action_space} != {env.action_space}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Observation spaces do not match: Box([-10.        -10.        -10.         -3.1415927  -3.1415927  -3.1415927\n -10.        -10.        -20.        -20.        -20.        -20.\n -20.        -20.       ], [10.        10.        10.         3.1415927  3.1415927  3.1415927\n 10.        10.        20.        20.        20.        20.\n 20.        20.       ], (14,), float32) != Dict('desired_goal': Box(-inf, inf, (2,), float32), 'observation': Box(-inf, inf, (20,), float32))"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import os\n",
        "import pandas as pd\n",
        "from stable_baselines3 import HerReplayBuffer, SAC\n",
        "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
        "\n",
        "\n",
        "path_save = \"/content/drive/MyDrive/Ant_custom/\"\n",
        "\n",
        "iteraciones = []\n",
        "recompensa_promedio=[]\n",
        "\n",
        "os.makedirs(path_save, exist_ok=True)\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive/Ant_custom/SAC_Ant.zip\"):\n",
        "    model = SAC.load(\"/content/drive/MyDrive/Ant_custom/SAC_Ant.zip\", env = env)\n",
        "    print(\"Model loaded\")\n",
        "else:\n",
        "    model = SAC('MlpPolicy', env,learning_rate=3e-4,policy_kwargs=policy_kwargs, verbose=1, batch_size=64)\n",
        "    print(\"Model created\")\n",
        "\n",
        "\n",
        "mean_rewards = []\n",
        "for i in range(10):\n",
        "  print(\"Training itteration \",i)\n",
        "  model.learn(total_timesteps=10000,log_interval = 10,callback=eval_callback)\n",
        "  # Save the agent\n",
        "\n",
        "  model.save(path_save+\"SAC_Ant_ayudame_dios\")\n",
        "  mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)\n",
        "  mean_rewards.append(mean_reward)\n",
        "  print(f'mean reward: {mean_reward}, std reward: +/-{std_reward}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "del model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wysYfH4R2pV"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjZBZhKMngcf"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import pybullet, pybullet_envs\n",
        "import torch as th\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "#Importamos librerías para realizar el multiprocesos y normalización del entorno\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "import gym\n",
        "import sys\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "sys.path.append('/content/ant_custom')\n",
        "import CustomAnt_Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwh9tOj5ngcg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#We begin with training but first we set some hyperparameters\n",
        "SEED                = 42\n",
        "NUM_ENVS            = 8\n",
        "ENV_ID              = 'AntCustomEnv-v0'\n",
        "HIDDEN_SIZE         = 256\n",
        "LEARNING_RATE       = 1e-4\n",
        "GAMMA               = 0.99\n",
        "GAE_LAMBDA          = 0.95\n",
        "PPO_EPSILON         = 0.2\n",
        "CRITIC_DISCOUNT     = 0.5\n",
        "ENTROPY_BETA        = 0.001\n",
        "PPO_STEPS           = 256\n",
        "MINI_BATCH_SIZE     = 64\n",
        "PPO_EPOCHS          = 10\n",
        "TEST_EPOCHS         = 10\n",
        "NUM_TESTS           = 10\n",
        "TARGET_REWARD       = 50\n",
        "MAX_STEPS           = 10000\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CEGi-c3P8jg",
        "outputId": "54063e25-1f9e-4157-ef7d-54679b75adb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import os\n",
        "import pandas as pd\n",
        "from stable_baselines3 import HerReplayBuffer, SAC\n",
        "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
        "\n",
        "env = gym.make(ENV_ID)\n",
        "n_sampled_goal = 4\n",
        "\n",
        "model = SAC(\n",
        "    \"MultiInputPolicy\",\n",
        "    env,\n",
        "    replay_buffer_class=HerReplayBuffer,\n",
        "    replay_buffer_kwargs=dict(\n",
        "      n_sampled_goal=n_sampled_goal,\n",
        "      goal_selection_strategy=\"final\",\n",
        "    ),\n",
        "    verbose=1,\n",
        "    buffer_size=int(1e6),\n",
        "    learning_rate=1e-3,\n",
        "    gamma=0.95,\n",
        "    batch_size=256,\n",
        "    policy_kwargs=dict(net_arch=[256, 256, 256]),learning_starts=11000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tieIpZusJggG",
        "outputId": "ecff982b-6958-4fb8-ad6b-a7ed48bd0a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training itteration  0\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 4.64e+03  |\n",
            "|    ep_rew_mean     | 2.01e+03  |\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 115       |\n",
            "|    time_elapsed    | 160       |\n",
            "|    total_timesteps | 18571     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.58e+04 |\n",
            "|    critic_loss     | 3.12e+06  |\n",
            "|    ent_coef        | 64.3      |\n",
            "|    ent_coef_loss   | -3.89     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7570      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 4.01e+03  |\n",
            "|    ep_rew_mean     | 914       |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 74        |\n",
            "|    time_elapsed    | 431       |\n",
            "|    total_timesteps | 32074     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.67e+04 |\n",
            "|    critic_loss     | 6.94e+07  |\n",
            "|    ent_coef        | 94.8      |\n",
            "|    ent_coef_loss   | 7.93      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 21073     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 4.34e+03  |\n",
            "|    ep_rew_mean     | 331       |\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 62        |\n",
            "|    time_elapsed    | 835       |\n",
            "|    total_timesteps | 52079     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.15e+04 |\n",
            "|    critic_loss     | 1.44e+08  |\n",
            "|    ent_coef        | 54.6      |\n",
            "|    ent_coef_loss   | -12       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 41078     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.89e+03  |\n",
            "|    ep_rew_mean     | 196       |\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 59        |\n",
            "|    time_elapsed    | 1041      |\n",
            "|    total_timesteps | 62206     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.81e+04 |\n",
            "|    critic_loss     | 5.93e+07  |\n",
            "|    ent_coef        | 50.2      |\n",
            "|    ent_coef_loss   | 3         |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 51205     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.71e+03  |\n",
            "|    ep_rew_mean     | 47.1      |\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 57        |\n",
            "|    time_elapsed    | 1285      |\n",
            "|    total_timesteps | 74230     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.79e+04 |\n",
            "|    critic_loss     | 9.17e+06  |\n",
            "|    ent_coef        | 58.7      |\n",
            "|    ent_coef_loss   | -2.25     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 63229     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.1e+03   |\n",
            "|    ep_rew_mean     | 38.7      |\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 57        |\n",
            "|    time_elapsed    | 1287      |\n",
            "|    total_timesteps | 74304     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.08e+05 |\n",
            "|    critic_loss     | 1.4e+07   |\n",
            "|    ent_coef        | 59.4      |\n",
            "|    ent_coef_loss   | 0.14      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 63303     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.01e+03  |\n",
            "|    ep_rew_mean     | -31.1     |\n",
            "| time/              |           |\n",
            "|    episodes        | 28        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1489      |\n",
            "|    total_timesteps | 84395     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.31e+05 |\n",
            "|    critic_loss     | 7.53e+07  |\n",
            "|    ent_coef        | 84.2      |\n",
            "|    ent_coef_loss   | 3.32      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 73394     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.04e+03  |\n",
            "|    ep_rew_mean     | -94.5     |\n",
            "| time/              |           |\n",
            "|    episodes        | 32        |\n",
            "|    fps             | 55        |\n",
            "|    time_elapsed    | 1750      |\n",
            "|    total_timesteps | 97383     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.69e+05 |\n",
            "|    critic_loss     | 1.2e+07   |\n",
            "|    ent_coef        | 91.8      |\n",
            "|    ent_coef_loss   | -0.0553   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 86382     |\n",
            "----------------------------------\n",
            "mean reward: -277.2057831999999, std reward: +/-813.4323256237659\n",
            "Training itteration  1\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.03e+03 |\n",
            "|    ep_rew_mean     | -289     |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 404      |\n",
            "|    time_elapsed    | 29       |\n",
            "|    total_timesteps | 12105    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.9e+05 |\n",
            "|    critic_loss     | 1.55e+07 |\n",
            "|    ent_coef        | 95.6     |\n",
            "|    ent_coef_loss   | 0.275    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 90104    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.05e+03  |\n",
            "|    ep_rew_mean     | -412      |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 89        |\n",
            "|    time_elapsed    | 271       |\n",
            "|    total_timesteps | 24369     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.63e+05 |\n",
            "|    critic_loss     | 3.34e+07  |\n",
            "|    ent_coef        | 74.1      |\n",
            "|    ent_coef_loss   | -2.15     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 102368    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.55e+03  |\n",
            "|    ep_rew_mean     | -373      |\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 77        |\n",
            "|    time_elapsed    | 392       |\n",
            "|    total_timesteps | 30550     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.38e+05 |\n",
            "|    critic_loss     | 8.87e+06  |\n",
            "|    ent_coef        | 78.7      |\n",
            "|    ent_coef_loss   | 0.194     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 108549    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.56e+03  |\n",
            "|    ep_rew_mean     | -395      |\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 68        |\n",
            "|    time_elapsed    | 597       |\n",
            "|    total_timesteps | 40972     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.54e+05 |\n",
            "|    critic_loss     | 9.73e+06  |\n",
            "|    ent_coef        | 75.2      |\n",
            "|    ent_coef_loss   | 2.75      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 118971    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.31e+03  |\n",
            "|    ep_rew_mean     | -367      |\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 66        |\n",
            "|    time_elapsed    | 700       |\n",
            "|    total_timesteps | 46241     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.33e+05 |\n",
            "|    critic_loss     | 8.18e+07  |\n",
            "|    ent_coef        | 72.5      |\n",
            "|    ent_coef_loss   | -4.77     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 124240    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.06e+03  |\n",
            "|    ep_rew_mean     | -326      |\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 763       |\n",
            "|    total_timesteps | 49461     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.32e+05 |\n",
            "|    critic_loss     | 6.64e+06  |\n",
            "|    ent_coef        | 74.9      |\n",
            "|    ent_coef_loss   | -0.773    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 127460    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.78e+03  |\n",
            "|    ep_rew_mean     | -283      |\n",
            "| time/              |           |\n",
            "|    episodes        | 28        |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 773       |\n",
            "|    total_timesteps | 49955     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.39e+05 |\n",
            "|    critic_loss     | 8.39e+06  |\n",
            "|    ent_coef        | 72.2      |\n",
            "|    ent_coef_loss   | -3.26     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 127954    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.88e+03  |\n",
            "|    ep_rew_mean     | -264      |\n",
            "| time/              |           |\n",
            "|    episodes        | 32        |\n",
            "|    fps             | 61        |\n",
            "|    time_elapsed    | 971       |\n",
            "|    total_timesteps | 60022     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.27e+05 |\n",
            "|    critic_loss     | 2.59e+07  |\n",
            "|    ent_coef        | 71.1      |\n",
            "|    ent_coef_loss   | 0.86      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 138021    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.1e+03   |\n",
            "|    ep_rew_mean     | -311      |\n",
            "| time/              |           |\n",
            "|    episodes        | 36        |\n",
            "|    fps             | 59        |\n",
            "|    time_elapsed    | 1278      |\n",
            "|    total_timesteps | 75521     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.31e+05 |\n",
            "|    critic_loss     | 7.14e+06  |\n",
            "|    ent_coef        | 68.8      |\n",
            "|    ent_coef_loss   | 1.85      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 153520    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.05e+03  |\n",
            "|    ep_rew_mean     | -297      |\n",
            "| time/              |           |\n",
            "|    episodes        | 40        |\n",
            "|    fps             | 58        |\n",
            "|    time_elapsed    | 1408      |\n",
            "|    total_timesteps | 82032     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.31e+05 |\n",
            "|    critic_loss     | 8.35e+06  |\n",
            "|    ent_coef        | 73.9      |\n",
            "|    ent_coef_loss   | 2.51      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 160031    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.13e+03  |\n",
            "|    ep_rew_mean     | -310      |\n",
            "| time/              |           |\n",
            "|    episodes        | 44        |\n",
            "|    fps             | 57        |\n",
            "|    time_elapsed    | 1637      |\n",
            "|    total_timesteps | 93680     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.38e+05 |\n",
            "|    critic_loss     | 4.69e+06  |\n",
            "|    ent_coef        | 68.2      |\n",
            "|    ent_coef_loss   | 0.468     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 171679    |\n",
            "----------------------------------\n",
            "mean reward: -307.9830234, std reward: +/-263.3067869231247\n",
            "Training itteration  2\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.77e+03  |\n",
            "|    ep_rew_mean     | -273      |\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 1325      |\n",
            "|    time_elapsed    | 8         |\n",
            "|    total_timesteps | 11073     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.39e+05 |\n",
            "|    critic_loss     | 1.64e+07  |\n",
            "|    ent_coef        | 69.6      |\n",
            "|    ent_coef_loss   | 3.67      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 178072    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.57e+03  |\n",
            "|    ep_rew_mean     | -157      |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 336       |\n",
            "|    time_elapsed    | 37        |\n",
            "|    total_timesteps | 12548     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.45e+05 |\n",
            "|    critic_loss     | 6.66e+06  |\n",
            "|    ent_coef        | 64.9      |\n",
            "|    ent_coef_loss   | -1.85     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 179547    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.65e+03  |\n",
            "|    ep_rew_mean     | -225      |\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 109       |\n",
            "|    time_elapsed    | 180       |\n",
            "|    total_timesteps | 19828     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.27e+05 |\n",
            "|    critic_loss     | 3.2e+07   |\n",
            "|    ent_coef        | 57.4      |\n",
            "|    ent_coef_loss   | -0.337    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 186827    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.14e+03  |\n",
            "|    ep_rew_mean     | -308      |\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 73        |\n",
            "|    time_elapsed    | 464       |\n",
            "|    total_timesteps | 34262     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.32e+05 |\n",
            "|    critic_loss     | 4.4e+07   |\n",
            "|    ent_coef        | 54.5      |\n",
            "|    ent_coef_loss   | 0.961     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 201261    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.33e+03  |\n",
            "|    ep_rew_mean     | -363      |\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 65        |\n",
            "|    time_elapsed    | 709       |\n",
            "|    total_timesteps | 46609     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.31e+05 |\n",
            "|    critic_loss     | 4.7e+06   |\n",
            "|    ent_coef        | 57.2      |\n",
            "|    ent_coef_loss   | -0.521    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 213608    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.44e+03  |\n",
            "|    ep_rew_mean     | -380      |\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 61        |\n",
            "|    time_elapsed    | 943       |\n",
            "|    total_timesteps | 58453     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.34e+05 |\n",
            "|    critic_loss     | 1.29e+07  |\n",
            "|    ent_coef        | 51.5      |\n",
            "|    ent_coef_loss   | -1.98     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 225452    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.11e+03  |\n",
            "|    ep_rew_mean     | -329      |\n",
            "| time/              |           |\n",
            "|    episodes        | 28        |\n",
            "|    fps             | 61        |\n",
            "|    time_elapsed    | 953       |\n",
            "|    total_timesteps | 58956     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.23e+05 |\n",
            "|    critic_loss     | 1.23e+07  |\n",
            "|    ent_coef        | 52.7      |\n",
            "|    ent_coef_loss   | -0.452    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 225955    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.98e+03  |\n",
            "|    ep_rew_mean     | -315      |\n",
            "| time/              |           |\n",
            "|    episodes        | 32        |\n",
            "|    fps             | 60        |\n",
            "|    time_elapsed    | 1040      |\n",
            "|    total_timesteps | 63244     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.24e+05 |\n",
            "|    critic_loss     | 6.75e+06  |\n",
            "|    ent_coef        | 50.2      |\n",
            "|    ent_coef_loss   | 0.233     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 230243    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.06e+03  |\n",
            "|    ep_rew_mean     | -340      |\n",
            "| time/              |           |\n",
            "|    episodes        | 36        |\n",
            "|    fps             | 59        |\n",
            "|    time_elapsed    | 1256      |\n",
            "|    total_timesteps | 74167     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.17e+05 |\n",
            "|    critic_loss     | 9.21e+06  |\n",
            "|    ent_coef        | 53.4      |\n",
            "|    ent_coef_loss   | 2.38      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 241166    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.35e+03  |\n",
            "|    ep_rew_mean     | -405      |\n",
            "| time/              |           |\n",
            "|    episodes        | 40        |\n",
            "|    fps             | 57        |\n",
            "|    time_elapsed    | 1651      |\n",
            "|    total_timesteps | 94173     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.06e+05 |\n",
            "|    critic_loss     | 7.65e+07  |\n",
            "|    ent_coef        | 51.4      |\n",
            "|    ent_coef_loss   | 2.54      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 261172    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.21e+03  |\n",
            "|    ep_rew_mean     | -377      |\n",
            "| time/              |           |\n",
            "|    episodes        | 44        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1714      |\n",
            "|    total_timesteps | 97324     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.31e+05 |\n",
            "|    critic_loss     | 8.39e+06  |\n",
            "|    ent_coef        | 52.3      |\n",
            "|    ent_coef_loss   | -1.26     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 264323    |\n",
            "----------------------------------\n",
            "mean reward: -91.6643454, std reward: +/-168.10283336209014\n",
            "Training itteration  3\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5.02e+03  |\n",
            "|    ep_rew_mean     | -510      |\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 106       |\n",
            "|    time_elapsed    | 188       |\n",
            "|    total_timesteps | 20068     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.28e+05 |\n",
            "|    critic_loss     | 7.96e+06  |\n",
            "|    ent_coef        | 49.6      |\n",
            "|    ent_coef_loss   | -1.26     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 276067    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.6e+03   |\n",
            "|    ep_rew_mean     | -469      |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 363       |\n",
            "|    total_timesteps | 28825     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.26e+05 |\n",
            "|    critic_loss     | 1.31e+07  |\n",
            "|    ent_coef        | 47.4      |\n",
            "|    ent_coef_loss   | -0.766    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 284824    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.4e+03  |\n",
            "|    ep_rew_mean     | -313     |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 79       |\n",
            "|    time_elapsed    | 363      |\n",
            "|    total_timesteps | 28830    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.3e+05 |\n",
            "|    critic_loss     | 9.58e+06 |\n",
            "|    ent_coef        | 47.4     |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 284829   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.15e+03  |\n",
            "|    ep_rew_mean     | -297      |\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 72        |\n",
            "|    time_elapsed    | 472       |\n",
            "|    total_timesteps | 34357     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.24e+05 |\n",
            "|    critic_loss     | 6.78e+06  |\n",
            "|    ent_coef        | 49        |\n",
            "|    ent_coef_loss   | -0.326    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 290356    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.07e+03  |\n",
            "|    ep_rew_mean     | -308      |\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 67        |\n",
            "|    time_elapsed    | 614       |\n",
            "|    total_timesteps | 41498     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.27e+05 |\n",
            "|    critic_loss     | 5.37e+06  |\n",
            "|    ent_coef        | 47        |\n",
            "|    ent_coef_loss   | -3.27     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 297497    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.73e+03  |\n",
            "|    ep_rew_mean     | -256      |\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 67        |\n",
            "|    time_elapsed    | 615       |\n",
            "|    total_timesteps | 41511     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.13e+05 |\n",
            "|    critic_loss     | 1.36e+07  |\n",
            "|    ent_coef        | 47.2      |\n",
            "|    ent_coef_loss   | 1.44      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 297510    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.05e+03 |\n",
            "|    ep_rew_mean     | -304     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 941      |\n",
            "|    total_timesteps | 57346    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.2e+05 |\n",
            "|    critic_loss     | 1.08e+07 |\n",
            "|    ent_coef        | 47.1     |\n",
            "|    ent_coef_loss   | 0.126    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 313345   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.19e+03 |\n",
            "|    ep_rew_mean     | -331     |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 58       |\n",
            "|    time_elapsed    | 1201     |\n",
            "|    total_timesteps | 70143    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.2e+05 |\n",
            "|    critic_loss     | 2.53e+07 |\n",
            "|    ent_coef        | 45.3     |\n",
            "|    ent_coef_loss   | -0.46    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 326142   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.05e+03  |\n",
            "|    ep_rew_mean     | -312      |\n",
            "| time/              |           |\n",
            "|    episodes        | 36        |\n",
            "|    fps             | 57        |\n",
            "|    time_elapsed    | 1272      |\n",
            "|    total_timesteps | 73704     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.13e+05 |\n",
            "|    critic_loss     | 7.4e+06   |\n",
            "|    ent_coef        | 44.4      |\n",
            "|    ent_coef_loss   | -0.498    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 329703    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.14e+03  |\n",
            "|    ep_rew_mean     | -310      |\n",
            "| time/              |           |\n",
            "|    episodes        | 40        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1511      |\n",
            "|    total_timesteps | 85543     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.08e+05 |\n",
            "|    critic_loss     | 9.49e+06  |\n",
            "|    ent_coef        | 48.8      |\n",
            "|    ent_coef_loss   | 2.08      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 341542    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.95e+03  |\n",
            "|    ep_rew_mean     | -282      |\n",
            "| time/              |           |\n",
            "|    episodes        | 44        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1512      |\n",
            "|    total_timesteps | 85612     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.23e+05 |\n",
            "|    critic_loss     | 9.51e+06  |\n",
            "|    ent_coef        | 48.8      |\n",
            "|    ent_coef_loss   | -0.296    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 341611    |\n",
            "----------------------------------\n",
            "mean reward: -818.0568433999999, std reward: +/-751.0662935559743\n",
            "Training itteration  4\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.75e+03  |\n",
            "|    ep_rew_mean     | -289      |\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 1539      |\n",
            "|    time_elapsed    | 7         |\n",
            "|    total_timesteps | 11003     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.08e+05 |\n",
            "|    critic_loss     | 3.61e+07  |\n",
            "|    ent_coef        | 46.4      |\n",
            "|    ent_coef_loss   | 1.85      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 356002    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.54e+03  |\n",
            "|    ep_rew_mean     | -177      |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 369       |\n",
            "|    time_elapsed    | 33        |\n",
            "|    total_timesteps | 12293     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.14e+05 |\n",
            "|    critic_loss     | 8.37e+06  |\n",
            "|    ent_coef        | 44.7      |\n",
            "|    ent_coef_loss   | 0.828     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 357292    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.86e+03 |\n",
            "|    ep_rew_mean     | -282     |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 94       |\n",
            "|    time_elapsed    | 235      |\n",
            "|    total_timesteps | 22298    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.2e+05 |\n",
            "|    critic_loss     | 5.38e+06 |\n",
            "|    ent_coef        | 42.9     |\n",
            "|    ent_coef_loss   | 1.25     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 367297   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.64e+03  |\n",
            "|    ep_rew_mean     | -461      |\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 66        |\n",
            "|    time_elapsed    | 631       |\n",
            "|    total_timesteps | 42261     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.17e+05 |\n",
            "|    critic_loss     | 8.83e+06  |\n",
            "|    ent_coef        | 40.1      |\n",
            "|    ent_coef_loss   | -0.84     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 387260    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.64e+03 |\n",
            "|    ep_rew_mean     | -473     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 62       |\n",
            "|    time_elapsed    | 841      |\n",
            "|    total_timesteps | 52716    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.1e+05 |\n",
            "|    critic_loss     | 7.84e+06 |\n",
            "|    ent_coef        | 39.5     |\n",
            "|    ent_coef_loss   | 2.32     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 397715   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.03e+03  |\n",
            "|    ep_rew_mean     | -549      |\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 58        |\n",
            "|    time_elapsed    | 1241      |\n",
            "|    total_timesteps | 72720     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.08e+05 |\n",
            "|    critic_loss     | 6.2e+06   |\n",
            "|    ent_coef        | 38.7      |\n",
            "|    ent_coef_loss   | 2.49      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 417719    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.16e+03  |\n",
            "|    ep_rew_mean     | -475      |\n",
            "| time/              |           |\n",
            "|    episodes        | 28        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1555      |\n",
            "|    total_timesteps | 88525     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.13e+05 |\n",
            "|    critic_loss     | 5.97e+06  |\n",
            "|    ent_coef        | 39.5      |\n",
            "|    ent_coef_loss   | -0.0197   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 433524    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.08e+03  |\n",
            "|    ep_rew_mean     | -478      |\n",
            "| time/              |           |\n",
            "|    episodes        | 32        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1755      |\n",
            "|    total_timesteps | 98531     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.02e+05 |\n",
            "|    critic_loss     | 3.29e+07  |\n",
            "|    ent_coef        | 39.5      |\n",
            "|    ent_coef_loss   | 2.13      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 443530    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.75e+03  |\n",
            "|    ep_rew_mean     | -426      |\n",
            "| time/              |           |\n",
            "|    episodes        | 36        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1761      |\n",
            "|    total_timesteps | 98839     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.14e+05 |\n",
            "|    critic_loss     | 7.14e+06  |\n",
            "|    ent_coef        | 40.6      |\n",
            "|    ent_coef_loss   | 1.57      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 443838    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.47e+03  |\n",
            "|    ep_rew_mean     | -384      |\n",
            "| time/              |           |\n",
            "|    episodes        | 40        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1761      |\n",
            "|    total_timesteps | 98849     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.15e+05 |\n",
            "|    critic_loss     | 7.12e+06  |\n",
            "|    ent_coef        | 40.5      |\n",
            "|    ent_coef_loss   | -1.85     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 443848    |\n",
            "----------------------------------\n",
            "mean reward: -311.08671879999997, std reward: +/-423.9548924946562\n",
            "Training itteration  5\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -547      |\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 106       |\n",
            "|    time_elapsed    | 187       |\n",
            "|    total_timesteps | 20015     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.09e+05 |\n",
            "|    critic_loss     | 1.14e+07  |\n",
            "|    ent_coef        | 37.8      |\n",
            "|    ent_coef_loss   | 0.114     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 454014    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 4.63e+03  |\n",
            "|    ep_rew_mean     | -561      |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 70        |\n",
            "|    time_elapsed    | 525       |\n",
            "|    total_timesteps | 37009     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.13e+05 |\n",
            "|    critic_loss     | 5.66e+06  |\n",
            "|    ent_coef        | 36.7      |\n",
            "|    ent_coef_loss   | 1.62      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 471008    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.92e+03  |\n",
            "|    ep_rew_mean     | -538      |\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 726       |\n",
            "|    total_timesteps | 47025     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.85e+04 |\n",
            "|    critic_loss     | 8.21e+06  |\n",
            "|    ent_coef        | 34.7      |\n",
            "|    ent_coef_loss   | 1.54      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 481024    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.04e+03  |\n",
            "|    ep_rew_mean     | -419      |\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 759       |\n",
            "|    total_timesteps | 48706     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.06e+05 |\n",
            "|    critic_loss     | 4.33e+06  |\n",
            "|    ent_coef        | 34.4      |\n",
            "|    ent_coef_loss   | -1.89     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 482705    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.44e+03  |\n",
            "|    ep_rew_mean     | -335      |\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 759       |\n",
            "|    total_timesteps | 48713     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.51e+04 |\n",
            "|    critic_loss     | 3.97e+06  |\n",
            "|    ent_coef        | 34.4      |\n",
            "|    ent_coef_loss   | -3.18     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 482712    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.18e+03  |\n",
            "|    ep_rew_mean     | -293      |\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 62        |\n",
            "|    time_elapsed    | 831       |\n",
            "|    total_timesteps | 52337     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.07e+05 |\n",
            "|    critic_loss     | 4.13e+06  |\n",
            "|    ent_coef        | 33        |\n",
            "|    ent_coef_loss   | 0.761     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 486336    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.94e+03  |\n",
            "|    ep_rew_mean     | -454      |\n",
            "| time/              |           |\n",
            "|    episodes        | 28        |\n",
            "|    fps             | 57        |\n",
            "|    time_elapsed    | 1432      |\n",
            "|    total_timesteps | 82415     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.54e+04 |\n",
            "|    critic_loss     | 4.28e+06  |\n",
            "|    ent_coef        | 33.4      |\n",
            "|    ent_coef_loss   | 1.59      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 516414    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.85e+03  |\n",
            "|    ep_rew_mean     | -452      |\n",
            "| time/              |           |\n",
            "|    episodes        | 32        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1607      |\n",
            "|    total_timesteps | 91243     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.09e+05 |\n",
            "|    critic_loss     | 4.37e+06  |\n",
            "|    ent_coef        | 31.8      |\n",
            "|    ent_coef_loss   | 0.555     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 525242    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.54e+03  |\n",
            "|    ep_rew_mean     | -404      |\n",
            "| time/              |           |\n",
            "|    episodes        | 36        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1613      |\n",
            "|    total_timesteps | 91521     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.05e+05 |\n",
            "|    critic_loss     | 5.52e+06  |\n",
            "|    ent_coef        | 32.2      |\n",
            "|    ent_coef_loss   | 0.686     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 525520    |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "iteraciones = []\n",
        "recompensa_promedio=[]\n",
        "mean_rewards = []\n",
        "path_save = \"/content/drive/MyDrive/Ant_custom/\"\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"Training itteration \",i)\n",
        "  model.learn(total_timesteps=100000)\n",
        "  # Save the agent\n",
        "  model.save(path_save+\"SAC_HER\")\n",
        "  mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)\n",
        "  mean_rewards.append(mean_reward)\n",
        "  print(f'mean reward: {mean_reward}, std reward: +/-{std_reward}')\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaP5-QapQ8B7",
        "outputId": "376bce0e-6903-41ae-9eca-3f2e0edf6da1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.02e+03  |\n",
            "|    ep_rew_mean     | -255      |\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 364       |\n",
            "|    time_elapsed    | 33        |\n",
            "|    total_timesteps | 12096     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.63e+04 |\n",
            "|    critic_loss     | 3.58e+05  |\n",
            "|    ent_coef        | 2.02      |\n",
            "|    ent_coef_loss   | -5.29     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 1095      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.84e+03  |\n",
            "|    ep_rew_mean     | -392      |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 88        |\n",
            "|    time_elapsed    | 257       |\n",
            "|    total_timesteps | 22688     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.14e+05 |\n",
            "|    critic_loss     | 1.05e+07  |\n",
            "|    ent_coef        | 98.7      |\n",
            "|    ent_coef_loss   | -0.992    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 11687     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.53e+03  |\n",
            "|    ep_rew_mean     | -387      |\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 72        |\n",
            "|    time_elapsed    | 417       |\n",
            "|    total_timesteps | 30400     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.06e+05 |\n",
            "|    critic_loss     | 8.34e+06  |\n",
            "|    ent_coef        | 92.6      |\n",
            "|    ent_coef_loss   | 0.119     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 19399     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.13e+03  |\n",
            "|    ep_rew_mean     | 248       |\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 60        |\n",
            "|    time_elapsed    | 834       |\n",
            "|    total_timesteps | 50112     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.87e+05 |\n",
            "|    critic_loss     | 3.99e+06  |\n",
            "|    ent_coef        | 166       |\n",
            "|    ent_coef_loss   | 4.15      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 39111     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.24e+03  |\n",
            "|    ep_rew_mean     | 99.3      |\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 56        |\n",
            "|    time_elapsed    | 1137      |\n",
            "|    total_timesteps | 64703     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.15e+05 |\n",
            "|    critic_loss     | 3.74e+08  |\n",
            "|    ent_coef        | 140       |\n",
            "|    ent_coef_loss   | 0.432     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 53702     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.98e+03  |\n",
            "|    ep_rew_mean     | 444       |\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 55        |\n",
            "|    time_elapsed    | 1279      |\n",
            "|    total_timesteps | 71506     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.88e+05 |\n",
            "|    critic_loss     | 7.52e+06  |\n",
            "|    ent_coef        | 137       |\n",
            "|    ent_coef_loss   | 3.69      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 60505     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.93e+03  |\n",
            "|    ep_rew_mean     | 359       |\n",
            "| time/              |           |\n",
            "|    episodes        | 28        |\n",
            "|    fps             | 54        |\n",
            "|    time_elapsed    | 1507      |\n",
            "|    total_timesteps | 82088     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.58e+05 |\n",
            "|    critic_loss     | 6.92e+06  |\n",
            "|    ent_coef        | 109       |\n",
            "|    ent_coef_loss   | -1.85     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 71087     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.91e+03  |\n",
            "|    ep_rew_mean     | 280       |\n",
            "| time/              |           |\n",
            "|    episodes        | 32        |\n",
            "|    fps             | 53        |\n",
            "|    time_elapsed    | 1737      |\n",
            "|    total_timesteps | 93244     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.43e+05 |\n",
            "|    critic_loss     | 5.89e+08  |\n",
            "|    ent_coef        | 113       |\n",
            "|    ent_coef_loss   | 4.63      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 82243     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.2e+03   |\n",
            "|    ep_rew_mean     | 139       |\n",
            "| time/              |           |\n",
            "|    episodes        | 36        |\n",
            "|    fps             | 52        |\n",
            "|    time_elapsed    | 2187      |\n",
            "|    total_timesteps | 115321    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.37e+05 |\n",
            "|    critic_loss     | 1.68e+07  |\n",
            "|    ent_coef        | 117       |\n",
            "|    ent_coef_loss   | -2.18     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 104320    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.27e+03  |\n",
            "|    ep_rew_mean     | 58        |\n",
            "| time/              |           |\n",
            "|    episodes        | 40        |\n",
            "|    fps             | 52        |\n",
            "|    time_elapsed    | 2497      |\n",
            "|    total_timesteps | 130903    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.36e+05 |\n",
            "|    critic_loss     | 6.62e+06  |\n",
            "|    ent_coef        | 106       |\n",
            "|    ent_coef_loss   | -1.2      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 119902    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3e+03     |\n",
            "|    ep_rew_mean     | 48.4      |\n",
            "| time/              |           |\n",
            "|    episodes        | 44        |\n",
            "|    fps             | 52        |\n",
            "|    time_elapsed    | 2523      |\n",
            "|    total_timesteps | 132099    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.29e+05 |\n",
            "|    critic_loss     | 8.01e+06  |\n",
            "|    ent_coef        | 109       |\n",
            "|    ent_coef_loss   | 0.527     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 121098    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.07e+03 |\n",
            "|    ep_rew_mean     | -17.6    |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 52       |\n",
            "|    time_elapsed    | 2828     |\n",
            "|    total_timesteps | 147280   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.4e+05 |\n",
            "|    critic_loss     | 1.66e+07 |\n",
            "|    ent_coef        | 89.3     |\n",
            "|    ent_coef_loss   | -4.47    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 136279   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.03e+03  |\n",
            "|    ep_rew_mean     | 5.68      |\n",
            "| time/              |           |\n",
            "|    episodes        | 52        |\n",
            "|    fps             | 51        |\n",
            "|    time_elapsed    | 3031      |\n",
            "|    total_timesteps | 157303    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.12e+05 |\n",
            "|    critic_loss     | 1.37e+07  |\n",
            "|    ent_coef        | 82.7      |\n",
            "|    ent_coef_loss   | 0.714     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 146302    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3.21e+03  |\n",
            "|    ep_rew_mean     | -73.9     |\n",
            "| time/              |           |\n",
            "|    episodes        | 56        |\n",
            "|    fps             | 51        |\n",
            "|    time_elapsed    | 3488      |\n",
            "|    total_timesteps | 179845    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.19e+05 |\n",
            "|    critic_loss     | 7.88e+06  |\n",
            "|    ent_coef        | 71.2      |\n",
            "|    ent_coef_loss   | -0.848    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 168844    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 3e+03     |\n",
            "|    ep_rew_mean     | -69       |\n",
            "| time/              |           |\n",
            "|    episodes        | 60        |\n",
            "|    fps             | 51        |\n",
            "|    time_elapsed    | 3488      |\n",
            "|    total_timesteps | 179865    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.23e+05 |\n",
            "|    critic_loss     | 9.99e+06  |\n",
            "|    ent_coef        | 71.1      |\n",
            "|    ent_coef_loss   | 4.08      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 168864    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.81e+03  |\n",
            "|    ep_rew_mean     | -64.7     |\n",
            "| time/              |           |\n",
            "|    episodes        | 64        |\n",
            "|    fps             | 51        |\n",
            "|    time_elapsed    | 3488      |\n",
            "|    total_timesteps | 179869    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.21e+05 |\n",
            "|    critic_loss     | 7.98e+06  |\n",
            "|    ent_coef        | 70.9      |\n",
            "|    ent_coef_loss   | 1.9       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 168868    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.7e+03   |\n",
            "|    ep_rew_mean     | -71.5     |\n",
            "| time/              |           |\n",
            "|    episodes        | 68        |\n",
            "|    fps             | 51        |\n",
            "|    time_elapsed    | 3561      |\n",
            "|    total_timesteps | 183491    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.27e+05 |\n",
            "|    critic_loss     | 1.2e+08   |\n",
            "|    ent_coef        | 69.4      |\n",
            "|    ent_coef_loss   | 0.445     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 172490    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.55e+03  |\n",
            "|    ep_rew_mean     | -68.6     |\n",
            "| time/              |           |\n",
            "|    episodes        | 72        |\n",
            "|    fps             | 51        |\n",
            "|    time_elapsed    | 3570      |\n",
            "|    total_timesteps | 183883    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.18e+05 |\n",
            "|    critic_loss     | 1.15e+07  |\n",
            "|    ent_coef        | 71.9      |\n",
            "|    ent_coef_loss   | -0.158    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 172882    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.43e+03 |\n",
            "|    ep_rew_mean     | -67.2    |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 3590     |\n",
            "|    total_timesteps | 184817   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.3e+05 |\n",
            "|    critic_loss     | 6.32e+06 |\n",
            "|    ent_coef        | 71.3     |\n",
            "|    ent_coef_loss   | -2.84    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 173816   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.56e+03 |\n",
            "|    ep_rew_mean     | -86.1    |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 3996     |\n",
            "|    total_timesteps | 204822   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.1e+05 |\n",
            "|    critic_loss     | 4.82e+06 |\n",
            "|    ent_coef        | 65.7     |\n",
            "|    ent_coef_loss   | -0.389   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 193821   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.62e+03  |\n",
            "|    ep_rew_mean     | -109      |\n",
            "| time/              |           |\n",
            "|    episodes        | 84        |\n",
            "|    fps             | 51        |\n",
            "|    time_elapsed    | 4316      |\n",
            "|    total_timesteps | 220407    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.25e+05 |\n",
            "|    critic_loss     | 4.25e+06  |\n",
            "|    ent_coef        | 58.7      |\n",
            "|    ent_coef_loss   | -0.586    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 209406    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.61e+03  |\n",
            "|    ep_rew_mean     | -122      |\n",
            "| time/              |           |\n",
            "|    episodes        | 88        |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 4509      |\n",
            "|    total_timesteps | 229872    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.27e+05 |\n",
            "|    critic_loss     | 4.73e+07  |\n",
            "|    ent_coef        | 61.4      |\n",
            "|    ent_coef_loss   | 1.21      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 218871    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.6e+03   |\n",
            "|    ep_rew_mean     | -125      |\n",
            "| time/              |           |\n",
            "|    episodes        | 92        |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 4696      |\n",
            "|    total_timesteps | 239067    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.15e+05 |\n",
            "|    critic_loss     | 1.4e+07   |\n",
            "|    ent_coef        | 64.9      |\n",
            "|    ent_coef_loss   | 1.34      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 228066    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.49e+03 |\n",
            "|    ep_rew_mean     | -120     |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 4698     |\n",
            "|    total_timesteps | 239181   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.1e+05 |\n",
            "|    critic_loss     | 7.72e+06 |\n",
            "|    ent_coef        | 63.7     |\n",
            "|    ent_coef_loss   | -0.274   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 228180   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.39e+03  |\n",
            "|    ep_rew_mean     | -115      |\n",
            "| time/              |           |\n",
            "|    episodes        | 100       |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 4703      |\n",
            "|    total_timesteps | 239382    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.08e+05 |\n",
            "|    critic_loss     | 2.78e+07  |\n",
            "|    ent_coef        | 64.2      |\n",
            "|    ent_coef_loss   | 0.394     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 228381    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.27e+03  |\n",
            "|    ep_rew_mean     | -105      |\n",
            "| time/              |           |\n",
            "|    episodes        | 104       |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 4703      |\n",
            "|    total_timesteps | 239391    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.24e+05 |\n",
            "|    critic_loss     | 5.61e+07  |\n",
            "|    ent_coef        | 64.3      |\n",
            "|    ent_coef_loss   | 0.764     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 228390    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.17e+03  |\n",
            "|    ep_rew_mean     | -84.1     |\n",
            "| time/              |           |\n",
            "|    episodes        | 108       |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 4703      |\n",
            "|    total_timesteps | 239395    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.19e+05 |\n",
            "|    critic_loss     | 1.13e+07  |\n",
            "|    ent_coef        | 64.3      |\n",
            "|    ent_coef_loss   | -2.33     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 228394    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 2.13e+03  |\n",
            "|    ep_rew_mean     | -76.4     |\n",
            "| time/              |           |\n",
            "|    episodes        | 112       |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 4785      |\n",
            "|    total_timesteps | 243424    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.22e+05 |\n",
            "|    critic_loss     | 7.4e+06   |\n",
            "|    ent_coef        | 66.2      |\n",
            "|    ent_coef_loss   | -1.38     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 232423    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.99e+03  |\n",
            "|    ep_rew_mean     | -173      |\n",
            "| time/              |           |\n",
            "|    episodes        | 116       |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 4908      |\n",
            "|    total_timesteps | 249520    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.22e+05 |\n",
            "|    critic_loss     | 1.54e+08  |\n",
            "|    ent_coef        | 62.8      |\n",
            "|    ent_coef_loss   | -1.98     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 238519    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.89e+03  |\n",
            "|    ep_rew_mean     | -159      |\n",
            "| time/              |           |\n",
            "|    episodes        | 120       |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 4999      |\n",
            "|    total_timesteps | 254007    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.14e+05 |\n",
            "|    critic_loss     | 8.48e+06  |\n",
            "|    ent_coef        | 66        |\n",
            "|    ent_coef_loss   | 2.14      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 243006    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.94e+03  |\n",
            "|    ep_rew_mean     | -262      |\n",
            "| time/              |           |\n",
            "|    episodes        | 124       |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 5228      |\n",
            "|    total_timesteps | 265445    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.19e+05 |\n",
            "|    critic_loss     | 7.44e+06  |\n",
            "|    ent_coef        | 68.9      |\n",
            "|    ent_coef_loss   | -1.5      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 254444    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1.84e+03  |\n",
            "|    ep_rew_mean     | -257      |\n",
            "| time/              |           |\n",
            "|    episodes        | 128       |\n",
            "|    fps             | 50        |\n",
            "|    time_elapsed    | 5235      |\n",
            "|    total_timesteps | 265834    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.19e+05 |\n",
            "|    critic_loss     | 1.82e+07  |\n",
            "|    ent_coef        | 69.8      |\n",
            "|    ent_coef_loss   | -0.176    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 254833    |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "model.learn(int(10e5))\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "model.save(\"her_sac_highway\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9hPQ2yZMANTN",
        "outputId": "33dd4fc6-b35c-4232-b500-5b8f39dbd8df"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(mean_rewards)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title('Mean Reward vs Iteration')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1d6XF2WDe-8Y31NTZGjkGCOyTeu2kSiir",
      "authorship_tag": "ABX9TyMWIzKm4usaIKJ/OaNaP7hB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}